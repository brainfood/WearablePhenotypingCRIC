---
title: 'Data analysis for "Time-Specific Associations of Wearable Sensor-Based
Cardiovascular and Behavioral Readouts with Disease Phenotypes in the Outpatient
Setting of the Chronic Renal Insufficiency Cohort"'
author: |
  | Nicholas F. Lahens, Mahboob Rahman, Jordana B. Cohen, Debbie L. Cohen, Jing Chen,
  | Matthew R. Weir, Harold I. Feldman, Gregory R. Grant, Raymond R. Townsend, & Carsten Skarke
date: "`r format(Sys.time(), '%m/%d/%y')`"
output:
    html_document:
        highlight: haddock
        code_folding: show
        toc: true
        toc_depth: 4
        number_sections: true
---
<!-- This helps warning messages stand out in the HTML output. It's primarily
     here to make the localization warning in the setup chunk stand out. Code
     from https://selbydavid.com/vignettes/alerts.html# -->
```{r setup_red_warning, include=FALSE}
knitr::knit_hooks$set(
   warning = function(x, options) {
     paste('\n\n<div class="alert alert-danger">',
           gsub('##', '\n', gsub('^##\ Warning:', '**Warning**', x)),
           '</div>', sep = '\n')
   }
)
```

```{r setup, echo=FALSE, warning=TRUE}
# By default, suppress messages from final document output.
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)

# This is used for constructing the html report from this document. We don't
# want to include this specific directory in the html output.
data_dir = "../DATA_DIR_FOR_GITHUB_TESTING"
```

This document is a companion to the manuscript entitled "[Time-Specific
Associations of Wearable Sensor-Based Cardiovascular and Behavioral Readouts
with Disease Phenotypes in the Outpatient Setting of the Chronic Renal
Insufficiency Cohort](https://doi.org/10.1177/20552076221107903)". It describes
how we analyzed the data for the manuscript.

The R code presented in this document uses the following R packages:
```{r load_libraries, warning=FALSE, message=FALSE, results='hide'}
library(sessioninfo)
library(nlme) # Used in Two-Way ANOVA analyses
library(multcomp) # Used for post-hoc tests
library(dendextend)
library(dplyr)
library(tidyr)
library(tibble)
library(readr)
library(lubridate)
library(ggplot2)
library(patchwork)
library(scales)
library(reshape2)
library(broom)
library(purrr)
```

# Preprocess Data

The raw data collected by the Zephyr BioPatch and generated by the Kubios
analysis software requires some pre-processing and aggregation before it is
suitably formatted for use in R (our tool of choice for these analyses). Here
we describe these pre-processing steps and provide the associated code.

For all of the R code, we define a directory containing all of the data (both
input and output). If the data directory is located in the same directory as the
Rmd file, set this variable to "./".
```{r define_data_dir, eval=FALSE}
data_dir = "./"
```

## Zephyr BioPatch Data

The Zephyr BioPatch collects information from several data streams, which are
stored across several files. For these analyses, we used data from Zephyr data
files with the following suffixes:

1. *_Accel.csv - Acceleration measured in three axes (vertical, lateral, and
   sagittal) at 100 Hz.
2. *_Breathing.csv - Breathing waveform data measured at 25 Hz.
3. *_Summary.csv - Assorted data streams (e.g. Posture, skin temp, device)
   measured at 1 Hz.

Our goal for pre-processing these data is to normalize them to a common
sampling frequency (1 minute) so they are more directly comparable, and to
aggregate data from all of these files into a single table.

### Parsing acceleration files

The acceleration files are a special case because they contain data sampled at
such a high frequency. The units of these acceleration values are 'bits', which
we'd like to convert to g (note, 1 g = 83 bits). Next, we want to aggregate
these data by minute. To do this, we will calculate the mean acceleration value
for each axis of motion across each minute. Lastly, we want a measure of general
acceleration across all three axes of motion at a given time. For this, we will
calculate the vector magnitude across the three axes.

$vector~magnitude = \sqrt{vertical_{accel}^2 + lateral_{accel}^2 + sagittal_{accel}^2}$

To perform all of these operations, we wrote a perl script
([parse_zephyr_accel_file.pl](./PERL/parse_zephyr_accel_file.pl)) which operates
on one *_Accel.csv file at a time. We used the following bash code and command
line perl to run this script on data from all subjects (assuming the files are
stored in separate directories by subject).
```{bash parse_zephyr_accel, eval=FALSE}
ls RAW_DATA/ZEPHYR/*/*_Accel.csv |
perl -ne '
    chomp();
    $current_file = $_;
    $new_file = $current_file;
    $new_file =~ s/\.csv$/.formatted_for_R.csv/g;
    print "perl ./PERL/parse_zephyr_accel_file.pl " .
          "$current_file > $new_file\n";
' > CRIC_study.Parse_zephyr_accel_files.sh

bash CRIC_study.Parse_zephyr_accel_files.sh
```

So this code will produce a new acceleration file with the *_Accel.formatted_for_R.csv
suffix, for each input *_Accel.csv file.


### Merge different Zephyr files by subject {#zephyr-preprocess}

Now that the acceleration files are properly formatted and aggregated by minute,
we want to aggregate the data from the *_Breathing.csv and &ast;_Summary.csv
files by minute and merge everything into a single table for each subject. To
accomplish this task, we wrote an R script
([process_and_plot_zephyr_data_by_subject.R](./R/process_and_plot_zephyr_data_by_subject.R)).

This script expects all of the data files for a given subject are stored
together in a directory names with the subject's identifier. In addition to
performing the aggregation/merge operations, this script also plots a
times-series of each Zephyr data stream for visual inspection and performs the
initial Cosinor fit and analysis (discussed in more detail [later in this
document](#cosinor-analyses)).

Again, we run this script on data from each subject through a combination of
bash code and command-line perl.
```{bash process_and_merge_raw_zephyr, eval=FALSE}
ls -d RAW_DATA/ZEPHYR/*/ |
perl -ne '
    chomp();
    $input_directory = $_;
    $subjectID = $input_directory;
    $subjectID =~ s/.*\/([^\/]+)\//$1/g;
    $output_directory = "PROCESSED_DATA/ZEPHYR/$subjectID/";
    $log_file = $output_directory . "Logfile.ZephyrBioPatch.Plotting_and_cosinor_analysis.$subjectID.txt";
    print "Rscript ./R/process_and_plot_zephyr_data_by_subject.R " .
          "$input_directory $output_directory &> $log_file\n";
' > CRIC_study.Plot_ZephyrBiopatch_and_perform_Cosinor_anlaysis.sh

bash CRIC_study.Plot_ZephyrBiopatch_and_perform_Cosinor_anlaysis.sh
```

This code will create several files in each of the subject data directories. The
*.pdf files contain raw time-series plots for different zephyr data streams. The
Aggregated_data.ZephyrBioPatch.[SubjectID].txt file is a tab-delimited table of
all the merged Zephyr data streams for a subject, aggregated by minute. The
Cosinor_results.ZephyrBioPatch.[SubjectID].txt file is a tab-delimited table of
the Cosinor fit parameters and results for each Zephyr data stream (discussed
[later in this document](#cosinor-analyses)). Lastly, the
Logfile.ZephyrBioPatch.Plotting_and_cosinor_analysis.[SubjectID].txt stores any
output to standard error, which includes a breakdown of the number of data
points with invalid/null values for each data stream.


## Kubios Results

The Kubios software suite reads the cardiac waveform data collected by the
Zephyr BioPatch and calculates various cardiac and heart rate variability (HRV)
metrics. Kubios saves the results of its analyses in raw text files in a
non-tabular format that's not readily readable by R. Here we describe the
procedure we used to reformat and merge the Kubios results, prior to any
downstream analyses.

Note: Kubios calculates the various HRV metrics at hour intervals, using all of
the Zephyr waveform data across the hour.

### Reformat Kubios results into tables {#kubios-preprocess}

We created a perl script
([parse_kubios_hrv_file.pl](./PERL/parse_kubios_hrv_file.pl)) to reformat the
Kubios data so it is more suitable for use with R. Just as before, we run this
script on the Kubios files for each subject using a combination of bash code and
command-line perl code.
```{bash parse_kubios_hrv, eval=FALSE}
ls RAW_DATA/KUBIOS/*/*_ECG_hrv.txt |
perl -ne 'chomp();
$current_file = $_;
$new_file = $current_file;
$new_file =~ s/\.txt$/.formatted_for_R.txt/g;
print "perl ./PERL/parse_kubios_hrv_file.pl " .
      "$current_file > $new_file\n";
' > CRIC_study.Parse_kubios_hrv_files.sh

bash CRIC_study.Parse_kubios_hrv_files.sh
```

### Changes introduced by Kubios v3.4.1 {#kubiosv3.4.1-changes}

We performed the analyses for this project using output from Kubios v3.0.2.
Subsequent versions of Kubios generate slightly different output files that will
require slight adjustments to the analyses performed below. Here we detail the
changes introduced by Kubios v3.4.1, the most recent version of Kubios available
as of summer 2020.

The most significant change, with respect to the analyses described below, is
how Kubios v3.4.1 reports the number of artifacts detected during each hour. In
Kubios v3.0.2, this field was labeled as "Artifacts corrected (%)." In Kubios
v3.4.1, this field is now labeled as "Beats corrected (%)." This affects the
analyses we present here, since we use these values to perform basic quality
filtering of our data. To update the code below to work with output from Kubios
v3.4.1, change all references to "Artifacts_(%)" or "Artifacts_corrected_(%)" to
match the new label "Beats_corrected_(%)."

In addition to changing the label for the artifacts corrected, Output files from
Kubios v3.4.1 report several additional metrics not previously reported by
Kubios v3.0.2. These include total number of heart beats in the study interval,
number of beats corrected, effective data length, PNS index, SNS index, and
Stress index. We've created an updated version or our parsing script
([parse_kubios_hrv_file.kubios_v3.4.1.pl](./PERL/parse_kubios_hrv_file.kubios_v3.4.1.pl))
to capture these additional metrics.


## CRIC Subject Metadata {#cric-metadata}

In addition to the data derived from the wearable biopatch device, we also have
metadata about the subjects' gender and ethnicity, biometric measurements like
height or blood LDL levels, and clinical classifications like Type 2 Diabetes
diagnosis. The diabetes diagnosis is particularly important for our analyses, as
we use that as the basis for breaking our subjects up into three cohorts:
subjects with chronic kidney disease (CKD), subjects with chronic kidney disease
and a type II diabetes mellitus diagnosis (CKD/T2DM), and the healthy controls.
Here we load and format these data tables for later use in these analyses.
```{r load_and_format_metadata}
# Table containing subject age, gender, and race/ethnicity
cric_subject_metadata = 
    readxl::read_xlsx(file.path(data_dir, "CLINICAL_AND_METADATA/CRIC_MetadataPennCleveland.xlsx")) %>%
    dplyr::select(-Comment) %>% 
    rename(SubjectID = PID,
           Clinic_Site = Cohort) %>% 
    select(SubjectID, Clinic_Site, Age, Gender, Ethnicity)

# Table of various biometric measurements, clinical classifications, class of
# drugs used for treatment. Broadly-speaking, the measurements in this table are
# either boolean (TRUE/FALSE) variables or numeric variables. These two variable
# types will be handled quite differently from one another, so we're splitting
# them into two tables.
cric_biometric_data =
    read_csv(file.path(data_dir, "CLINICAL_AND_METADATA/CRIC_biometrics.csv"),
             col_types = cols(pid = col_character(), .default = col_double())) %>% 
    rename(SubjectID = pid)

cric_biometric_data.boolean_variables =
    cric_biometric_data %>% 
    # Identify boolean variables as those only use the values 0, 1, or NA.
    select(SubjectID, function(x) all(unique(x) %in% c(0,1,NA))) %>% 
    pivot_longer(cols = -SubjectID, names_to = "Metric", values_to = "Value") %>% 
    mutate(Value = as.logical(Value))

cric_biometric_data.numeric_variables =
    cric_biometric_data %>% 
    # Identify numeric variables as those that use values other than 0, 1, or NA.
    select(SubjectID, function(x) !all(unique(x) %in% c(0,1,NA))) %>% 
    pivot_longer(cols = -SubjectID, names_to = "Metric", values_to = "Value")

# Since we make heavy use of the diabetes diagnosis throughout these analyses,
# we separate this mapping into its own mini-table for quick reference. This
# also allows us to update the variable from a boolean, to one that supports
# three possible values: CKD, CKD/T2DM, Healthy control.

cohort_levels = c("CKD", "CKD/T2DM", "Healthy control")

cric_diabetic_designation =
    cric_biometric_data.boolean_variables %>%
    filter(Metric == "diabetes") %>% 
    mutate(Cohort = ifelse(Value, "CKD/T2DM", "CKD")) %>% 
    select(SubjectID, Cohort) %>% 
    bind_rows(read_csv(file.path(data_dir, "CLINICAL_AND_METADATA/HealthyControls.csv"),
                       col_types = cols(.default = col_character()))) %>% 
    mutate(Cohort = factor(Cohort, cohort_levels)) %>% 
    arrange(Cohort)
```


# Variance Correlation Analysis ('Variance Explained' - VE)

In our [previous work](https://pubmed.ncbi.nlm.nih.gov/29215023/), we explored
the linear relationships between different variables from remotely-collected
data, we calculated the linear regression coefficient of determination ($R^{2}$)
between each pair of wearable data variables. These $R^{2}$ values allowed us to
assess the proportion of variance in each variable explained by all other
variables. Here we repeat these analyses on both the Zephyr BioPatch and Kubios
metrics.

In this document, we use the phrases "variance correlation" and "variance
explained" (VE) interchangeably to refer to these analyses. We will perform
these VE analyses at various levels (by cohort, by individual subject) using
different sets of variables (just Zephyr, just Kubios, both Zephyr and Kubios)
to explore how these choices affect the relationships between the variables.

## Merge and Format Data

We need to perform some additional pre-processing steps in order to properly
integrate the Zephyr and Kubios data, prior to performing the pairwise
regression between all variables. In order to compare specific Kubios and Zephyr
measurements to each other, we need to develop a common index between the two
data types. To accomplish this we will index each measurement by subject ID and
time, defined as minutes since midnight (00:00) on the first day of measurements
for the corresponding subject.

Since we have Kubios measurements at every hour and Zephyr measurements at every
minute, we will need to aggregate the Zephyr measurements by hour to make them
directly comparable. We will also apply a quality filter to the Kubios data to
remove measurements Kubios flagged as potential artifacts. These pre-processing
and indexing steps are described below. Note, the quality filtering code was
written to work with output from Kubios v3.0.1. If you wish to use it with a
later version (v3.4.1), please read the [note above](#kubiosv3.4.1-changes).

Note, we use the data tables we create here as the basis for other analyses
throughout this document. Also, in all cases we create a column in the formatted
files that stored the subject ID, even though this information is contained in
the time/subject index. Maintaining a separate subject ID column makes it easier
to quickly filter the data tables by cohort, or for specific subjects.


### Zephyr BioPatch data

We will generate two sets of Zephyr BioPatch data formatted for the VE analyses.
For the analyses just using the Zephyr data we want to maintain the full
fidelity of the data, so we won't perform any additional data aggregation. For
analyses including both the Zephyr and Kubios data we will need to aggregate the
Zephyr data by hour in order to properly match the Kubios data, which was
calculated for every hour.

As before, we've written an R script
([format_zephyr_data_for_variance_explained_by_subject.R](./R/format_zephyr_data_for_variance_explained_by_subject.R))
that performs both the formatting and aggregation operations given the Zephyr
data files [prepared above](#zephyr-preprocess).

#### Aggregate by minute

We'll use the following bash code and command-line perl to run this script on
the pre-processed Zephyr data for each subject. Note, we pass "mins" as the
second argument to the script to indicate we want to aggregate these data by
minute. They are already aggregated by minute, so this script will just be
changing the format of these data.
```{bash format_zephyr_for_ve_by_min, eval=FALSE}
ls -d PROCESSED_DATA/ZEPHYR/*/ | 
perl -ne '
    chomp();
    $input_directory = $_;
    print "Rscript ./R/format_zephyr_data_for_variance_explained_by_subject.R " .
          "$input_directory \"mins\"\n";
' > CRIC_study.Format_ZephyrBiopatch_for_variance_explained.agg_by_min.sh

bash CRIC_study.Format_ZephyrBiopatch_for_variance_explained.agg_by_min.sh
```

This code will create a
Formatted_for_variance_explained.ZephyrBioPatch.[SubjectID].agg_by_mins.txt file
in each subject's data directory. This tab-delimited file contains all of the
Zephyr measurements for the subject indexed by the subject's ID and the number
of minutes since midnight on the first day of measurements. For example, an
index value of 3504_Sub123456 indicates the measurement was collected from
subject 123456 at 10:24 AM on the third day of measurements (3504 minutes = 2
days + 10 hours + 24 minutes). This file stores data in a 'wider' format, so the
values for each data stream are stored in their own columns. So all of the data
stored in a row were collected by the Zephyr BioPatch device over the same time
period, identified by the index.


#### Aggregate by hour

To aggregate these data by hour we will use the same script as above, except
this time we will pass "hours" as the second argument. For each data stream,
this script will then calculate the mean value of all measurements collected
during each hour. Otherwise, the output of this code will be the same as above
(aside from the reduced number of lines in the file, since the 60 measurements
across each hour will be collapsed into a single line)
```{bash format_zephyr_for_ve_by_hour, eval=FALSE}
ls -d PROCESSED_DATA/ZEPHYR/*/ | 
perl -ne '
    chomp();
    $input_directory = $_;
    print "Rscript ./R/format_zephyr_data_for_variance_explained_by_subject.R " .
          "$input_directory \"hours\"\n";
' > CRIC_study.Format_ZephyrBiopatch_for_variance_explained.agg_by_hour.sh

bash CRIC_study.Format_ZephyrBiopatch_for_variance_explained.agg_by_hour.sh
```

#### Merge data from all subjects

So now that the data are appropriately formatted and aggregated for each
subject, we need to combine all of these data into a single table to perform the
VE analysis.

For the data aggregated by minute:
<!-- There is a suppressed warning here. The CRIC subjects don't have a 'Misc'
     column and will generate a lot of parser warnings. -->
```{r merge_zephyr_agg_by_min}
zephyr_formatted_files =
    list.files(path = file.path(data_dir, "PROCESSED_DATA/"),
               recursive = TRUE, full.names = TRUE,
               pattern = "Formatted_for_variance_explained\\.ZephyrBioPatch\\..*\\.agg_by_mins\\.txt")

merged_zephyr_data.for_VE = tibble()

for(file in zephyr_formatted_files) {

    #Extract subject ID from filename path
    subjectID = gsub(".*/([^/]+)/Formatted_for_variance_explained.*","\\1", file)

    #Load and aggregate data
    merged_zephyr_data.for_VE =
        merged_zephyr_data.for_VE %>%
        bind_rows(
            read_tsv(file,
                     col_types = cols(
                         .default = col_double(),
                         TimeSubjectIndex = col_character(),
                         Misc = col_character()
                     )) %>%
                mutate(SubjectID = subjectID) %>%
                # The Misc column is only used by the control subject to
                # identify the device used to collect the data (since the
                # controls alternated between using two devices). Moving
                # forward, we don't need to maintain this information.
                select(SubjectID, everything(), -any_of("Misc"))
        )
}
```

For the data aggregated by hour:
<!-- There is a suppressed warning here. The CRIC subjects don't have a 'Misc'
     column and will generate a lot of parser warnings. -->
```{r merge_zephyr_agg_by_hour}
zephyr_formatted_files =
    list.files(path = file.path(data_dir, "PROCESSED_DATA/"),
               recursive = TRUE, full.names = TRUE,
               pattern = "Formatted_for_variance_explained\\.ZephyrBioPatch\\..*\\.agg_by_hours\\.txt")

merged_zephyr_data.by_hour.for_VE = tibble()

for(file in zephyr_formatted_files) {

    #Extract subject ID from filename path
    subjectID = gsub(".*/([^/]+)/Formatted_for_variance_explained.*","\\1", file)

    #Load and aggregate data
    merged_zephyr_data.by_hour.for_VE =
        merged_zephyr_data.by_hour.for_VE %>%
        bind_rows(
            read_tsv(file,
                     col_types = cols(
                         .default = col_double(),
                         TimeSubjectIndex = col_character(),
                         Misc = col_character()
                     )) %>%
                mutate(SubjectID = subjectID) %>%
                # The Misc column is only used by the control subject to
                # identify the device used to collect the data (since the
                # controls alternated between using two devices). Moving
                # forward, we don't need to maintain this information.
                select(SubjectID, everything(), -any_of("Misc"))
        )
}
```

### Kubios results

To prepare the Kubios results for the VE analyses, we will need to combine the
data from all subjects into a single table, and index each measurement using the
same scheme as the Zephyr BioPatch data ([minutes since midnight on first
day]_[subject ID]). Note, there are a few differences between the way we hand
the Kubios and Zephyr data files. First, the Kubios measurements were calculated
for every hour, so we do not need to perform any aggregations operations.
Second, when running Kubios we needed to process the data from each subject in
batches, which resulted in multiple Kubios result files for each subject. We
need to properly account for and merge the multiple data files for each subject.

We start by merging all of the formatted Kubios result files we [prepared
above](#kubios-preprocess). Note, the Kubios measurement were calculated every
hour, so we do not need to perform any aggregation operations.
```{r merge_kubios}
#Get list of all files
kubios_filenames = list.files(path = file.path(data_dir, "RAW_DATA/KUBIOS/"),
                              pattern = ".*\\.formatted_for_R\\.txt",
                              recursive = TRUE, full.names = TRUE)

#Read each file and collect the results
merged_kubios_results = tibble()

for(file in kubios_filenames) {

    # Extract subject ID from the directory name
    subjectID = gsub(".*/KUBIOS/+([^/]+)/+.*", "\\1", file)

    merged_kubios_results =
        merged_kubios_results %>%
        bind_rows(read_tsv(file,
                           locale = locale(tz="America/New_York"),
                           col_types =
                               cols(
                                   .default = col_double(),
                                   SampleIDs = col_character(),
                                   Measurement_Start = col_datetime(format = "%d/%m/%y %H:%M:%S"),
                                   Measurement_Periods = col_character(),
                                   `NNxx_(beats)...Time_Domain_Results...Statistical_parameters` = col_integer(),
                                   `Max_line_length_(beats)...Nonlinear_Results...Recurrence_plot_analysis` = col_integer()
                               )) %>%
                      # The kubios results files for the control subjects had a
                      # different name for the Artifacts_corrected column. Change
                      # this so the CRIC and Control tables use the same column
                      # names. The plyr version of the rename function makes it
                      # easy to rename a column only if it exists.
                      plyr::rename(c("Artifacts_(%)" = "Artifacts_corrected_(%)"),
                                   warn_missing = FALSE) %>% 
                      filter(!is.na(SampleIDs)) %>%
                      mutate(SubjectID = subjectID) %>%
                      select(SubjectID, Measurement_Start, SampleIDs, Measurement_Periods, everything()))
}

# The Kubios analyses introduced duplicate rows into the results file that have
# are completely identical, except for the entry in the "SampleIDs" file. This
# is an artifact that we remove by dropping the SampleIDs column and filtering
# out duplicate rows. Additionally, there are a handful of rows that contain
# Kubios metrics calculated from partial hours (e.g. measurement period from
# "31:37:24-32:37:23"). We want to exclude these as well.
merged_kubios_results =
    merged_kubios_results %>%
    select(-SampleIDs) %>%
    distinct() %>%
    filter(grepl(":(00|59):", Measurement_Periods))
```

When performing calculations, Kubios can flag various measurements as artifacts
for a variety of reasons (e.g. reported values or changes in values not
physiologically possible). Next, Kubios will attempt to correct these artifacts
and records the percentage of measurements within a given hour that required
correction. This information is reported in the "Artifacts_corrected_(%)" column
of the formatted Kubios result tables. In order to limit the impact of invalid
data on our results, we exclude Kubios data from any measurement periods where
more than 5% of the underlying data were flagged as artifacts.

Note, this code was written to work with output from Kubios v3.0.1. If you wish
to use it with a later version (v3.4.1), please read the [note
above](#kubiosv3.4.1-changes).

Here we apply this filter, index the measurements by time and subject, and
format the merged table of Kubios results for use in the VE analyses below.
```{r filter_and_format_kubios_for_ve}
filtered.merged_kubios_results.for_VE =
    merged_kubios_results %>%
    filter(`Artifacts_corrected_(%)` <= 5) %>%
    separate(Measurement_Periods, into=c("Period_Start", "Period_Stop"),
             sep="-", remove = FALSE) %>%
    # The Kubios results already report measurement times as hours since the start
    # of the first measurement day in the file. Since we have multiple Kubios
    # result files for most subjects, we need to represent the measurement times
    # as hours since the first day of a given subject's measurements across all
    # files. To do this for a given subject, we find the date of the earliest
    # measurement across all of a subject's Kubios result files. Then, for each
    # file we calculate an offset, in minutes, between the start date for that
    # file and the date of the earliest measurement.
    group_by(SubjectID) %>%
    mutate(
        # Converting the start date to a lubridate object makes some of the date
        # arithmetic below easier.
        File_StartDate = lubridate::floor_date(Measurement_Start, unit = "days"),
        Subject_StartDate = min(File_StartDate, na.rm=TRUE),
        Start_day_offset = as.integer(difftime(File_StartDate, Subject_StartDate, units = "mins")),
        # Convert measurement time from hours since midnight on the file's start
        # date to minutes since midnight on the file's start date.
        File_TimeIndex = as.numeric(as.duration(hms(Period_Start)), "minutes"),
        TimeIndex = File_TimeIndex + Start_day_offset,
        TimeSubjectIndex = paste0(TimeIndex, "_", SubjectID)
    ) %>%
    ungroup() %>%
    arrange(SubjectID, File_StartDate, File_TimeIndex) %>%
    select(SubjectID, TimeSubjectIndex,
           `Mean_HR_(beats/min)...Time_Domain_Results...Statistical_parameters`, everything(),
           # Exclude columns I won't need in downstream analyses
           -Measurement_Start:-`Artifacts_corrected_(%)`,
           -File_StartDate:-TimeIndex)
```

### Add Additional Time Columns and Cohort To Formatted Tables {#add-tod-and-cohort-columns}

When assembling the tables above we took a minimalist approach, just keeping a
subject ID, a time/subject index, and then the various data columns. For some of
the later analyses, like the [Two-Way ANOVA](#two-way-anova-analysis), it is
useful to have additional information, like a subject's cohort (CKD, CKD/T2DM,
Healthy control) or a day/night label indicating the rough time of day when each
measurement was taken. Rather than create separate tables with this information,
we'll just add these columns to the existing tables. We will also use this
opportunity to modify the Posture variable from the Zephyr BioPatch, and drop
the skin temperature variable, since the sensor was not calibrated correctly.

For each time point, we want to record the clock hour (0-24), and label the
measurement time as either day (7:00 AM - 9:59 PM) or night (10:00 PM - 6:59
AM).

***
**A Note About The Posture Variable:**

The Posture metric is on a scale from -100 (supine) to 0 (vertical) to +50
(prone). Since both extremes represent someone lying down, we would also like to
represent this on a scale from standing to lying down. To do this, we'll take
the absolute value of the Posture value, that way it just becomes deviation from
vertical/standing.

***

We'll start by adding the additional columns to the Zephyr BioPatch data,
calculating the absolute posture metric, and dropping the skin temp. We're doing
this for data aggregated by both minute and hour.
```{r add_tod_and_posture_to_formatted_zephyr}
merged_zephyr_data.for_VE =
    merged_zephyr_data.for_VE %>% 
    # Add cohort label to each subject
    left_join(cric_diabetic_designation, by = "SubjectID") %>% 
    # Update posture variable
    rename(Posture.Original = Posture) %>% 
    mutate(Posture = abs(Posture.Original)) %>% 
    # Only want the time index, which is the first thing before the underscore.
    # So we can drop everything else. Also, maintain the original
    # TimeSubjectIndex column.
    separate(TimeSubjectIndex, into = c("TimeIndex"), sep = "_", extra = "drop",
             convert = TRUE, remove = FALSE) %>% 
    #Calculate clock hours from the TimeIndex (they are currently minutes since
    #midnight of the first measurement day). Also, label times between 10 PM
    #and 6AM as arbitrary "Night".
    mutate(HourIndex = TimeIndex / 60,
           ClockHour = HourIndex %% 24,
           Time_of_day = case_when(ClockHour >= 22 | ClockHour < 7 ~ "Night",
                                   TRUE ~ "Day"),
           Time_of_day = factor(Time_of_day, levels = c("Day", "Night"))) %>%
    select(SubjectID, Cohort, TimeSubjectIndex, ClockHour, Time_of_day,
           everything(), -TimeIndex, -HourIndex, -SkinTemp)

merged_zephyr_data.by_hour.for_VE =
    merged_zephyr_data.by_hour.for_VE %>% 
    # Add cohort label to each subject
    left_join(cric_diabetic_designation, by = "SubjectID") %>% 
    # Update posture variable
    rename(Posture.Original = Posture) %>% 
    mutate(Posture = abs(Posture.Original)) %>% 
    # Only want the time index, which is the first thing before the underscore.
    # So we can drop everything else. Also, maintain the original
    # TimeSubjectIndex column.
    separate(TimeSubjectIndex, into = c("TimeIndex"), sep = "_", extra = "drop",
             convert = TRUE, remove = FALSE) %>% 
    #Calculate clock hours from the TimeIndex (they are currently minutes since
    #midnight of the first measurement day). Also, label times between 10 PM
    #and 6AM as arbitrary "Night".
    mutate(HourIndex = TimeIndex / 60,
           ClockHour = HourIndex %% 24,
           Time_of_day = case_when(ClockHour >= 22 | ClockHour < 7 ~ "Night",
                                   TRUE ~ "Day"),
           Time_of_day = factor(Time_of_day, levels = c("Day", "Night"))) %>%
    select(SubjectID, Cohort, TimeSubjectIndex, ClockHour, Time_of_day,
           everything(), -TimeIndex, -HourIndex, -SkinTemp)
```

And now the Kubios data.
```{r add_tod_and_posture_to_formatted_kubios}
filtered.merged_kubios_results.for_VE =
    filtered.merged_kubios_results.for_VE %>% 
    # Add cohort label to each subject
    left_join(cric_diabetic_designation, by = "SubjectID") %>% 
    # Only want the time index, which is the first thing before the underscore.
    # So we can drop everything else. Also, maintain the original
    # TimeSubjectIndex column.
    separate(TimeSubjectIndex, into = c("TimeIndex"), sep = "_", extra = "drop",
             convert = TRUE, remove = FALSE) %>% 
    #Calculate clock hours from the TimeIndex (they are currently minutes since
    #midnight of the first measurement day). Also, label times between 10 PM
    #and 6AM as arbitrary "Night".
    mutate(HourIndex = TimeIndex / 60,
           ClockHour = HourIndex %% 24,
           Time_of_day = case_when(ClockHour >= 22 | ClockHour < 7 ~ "Night",
                                   TRUE ~ "Day"),
           Time_of_day = factor(Time_of_day, levels = c("Day", "Night"))) %>%
    select(SubjectID, Cohort, TimeSubjectIndex, ClockHour, Time_of_day,
           everything(), -TimeIndex, -HourIndex)
```


### Combine Zephyr data and Kubios results

Lastly, we are combining the Zephyr BioPatch data and Kubios results into a
single, large table. This is for the VE analyses that compare both Zephyr and
Kubios measurements to each other. Since we prepared both the Zephyr and Kubios
data with a common indexing scheme above, this merge operation is relatively
simple. We keep and combine measurements from all timepoints that have both
valid Zephyr and Kubios data.
```{r merge_zephyr_and_kubios_for_ve}
merged_zephyr_and_kubios_data.by_hour.for_VE =
    inner_join(
        filtered.merged_kubios_results.for_VE,
        merged_zephyr_data.by_hour.for_VE,
        by = c("SubjectID", "Cohort", "TimeSubjectIndex", "ClockHour", "Time_of_day"))
```


## Perform Variance Correlation Analyses

Now that we have properly formatted and indexed all the Zephyr and Kubios data,
we will perform the Variance Correlation Analyses. Briefly, we perform a linear
regression between each pair of variables, using the *lm* function in R. The
degree to which the variability in one variable is responsible for the
variability in another is quantified by the $R^2$ (coefficient of determination)
value from the linear regression. The code for performing these analyses is
provided in the following sections.

### Functions for variance correlation analyses

Here are several R functions that perform the variance correlation analyses and
plot the results as heatmaps. These are modified versions of functions
originally written by Amy Campbell as part of our [previous
work](https://pubmed.ncbi.nlm.nih.gov/29215023/). You can find the originals
[here](https://github.com/itmat/chronobiome/blob/master/Variance_Correlation_Plot.R).

The roxygen comments before each function describe their purpose, arguments, and
output. Click the 'Code' button to the right if you want to view the functions.
```{r define_ve_functions, class.source="fold-hide"}
#' Generates a simple linear regression model of var1~var2, and returns
#' R-squared for that model to represent proportion of V1 explained by V2
#'
#' @param var1 n x 1 matrix of data to 'be explained' by var2. n = number of
#'   observations for the given variable. These observations should match those
#'   in var 2.
#' @param var2 n x 1 matrix of data to 'explain' var1. n = number of
#'   observations for the given variable. These observations should match those
#'   in var 1.
#' @param return_pvalue boolean indicating whether or not function returns
#'   p-value for the fit of the estimated linear model to the data, instead of
#'   the R-squared value: FALSE (default) - return R-squared value. TRUE -
#'   return p-value from fit.
#'
#' @return A vector containing the name of variable 1
GetR2 <- function(var1,
                  var2,
                  return_pvalue=FALSE) {
    
    Var1name <- colnames(var1)[1]
    Var2name <-  colnames(var2)[1]
    model <- NA
    ret_value <- NA
    
    #Skip lm if either input vector consists only of NA's.
    if( !(all(is.na(var1))) && !(all(is.na(var2))) ) {
        
        model <- lm(as.numeric(var1) ~ as.numeric(var2))
        ret_value <- summary(model)$r.squared
        
    }
    
    if(return_pvalue && !(is.na(model))) {
        #Calculate the p-value using F-statistic attributes from linear model
        fstats <- summary(model)$fstatistic
        
        ret_value <- NA
        
        #If an f-test was performed successfully
        if(!is.null(fstats)) {
            pvalue <- pf(fstats[1],fstats[2],fstats[3],lower.tail=F)
            attributes(pvalue) <- NULL
            ret_value <- pvalue
        }
        
    }
    
    return(c(Var1name, Var2name, ret_value))
}


#' Generate dataframe of the % variance explained (R2) for each variable by each
#' other variable.
#'
#' @param input_data Dataframe with one data type per column. Each column in the
#'   dataframe will be compared to each other column (and itself). Because these
#'   will be compared using R-squared values yielded from simple linear
#'   regression models, they must be numeric. All other columns (e.g. time
#'   stamps, subject IDs, etc.) will be removed.
#' @param color_by_pvalue boolean indicating whether to return a matrix of
#'   p-values (TRUE), the R**2 values (FALSE) from the linear model fit.
#'
#' @return DataFrame containing all of the pairwise R**2 values calculated from
#'   the input data. The factor.1 and factor.2 columns list names of the two
#'   variables being regressed against each other. Depending upon the user's
#'   choice for the color_by_pvalue parameter, the third column contains either
#'   the R**2 or p-value from the F test for significance from the linear
#'   regression.
getR2matrix <- function(input_data,
                        pvalue_matrix=FALSE) {
    
    r2Matrix <- c()
    for (a in colnames(input_data)) {
        for (b in colnames(input_data)) {
            r2Matrix <-
                rbind(r2Matrix,
                      GetR2(as.matrix(input_data[a]),
                            as.matrix(input_data[b]),
                            pvalue_matrix
                      ))
        }
    }
    
    r2.data_frame <- data.frame(r2Matrix)
    colnames(r2.data_frame) <- c("factor.1", "factor.2", "variability.explained")
    r2.data_frame$variability.explained <-
        as.numeric(as.character(r2.data_frame$variability.explained))
    
    return(r2.data_frame)
    
}


#' Generates geom_tile plot of the % variance explained (R2) for each variable
#' by each other variable.
#'
#' @param input_data Dataframe to plot, with one data type per column. Each
#'   column in the dataframe will be compared to each other column (and itself).
#'   Because these will be compared using R-squared values yielded from simple
#'   linear regression models, they must be numeric. All other columns (e.g.
#'   time stamps, subject IDs, etc.) will be removed.
#' @param title title to display on plot.
#' @param variable_ordering Vector of strings specifying the plot order of the
#'   variables (derived from column names) in the graph. If NULL (default),
#'   variables are plotted in alphabetical order.
#' @param triangle_heatmap boolean indicating whether to plot just the top of
#'   the heatmap, above the diagonal. The heatmap is mirrored across the
#'   diagonal, so plotting the full heatmap technically contains redundant
#'   information.
#' @param add_labels boolean indicating whether to add text labels to each box
#'   in the heatmap that list the R-squared values. The listed values are
#'   rounded to two decimal places.
#' @param color_by_pvalue boolean indicating whether to color the heatmap using
#'   p-values from the fit of the lm model, instead of the R-squared values. The
#'   colors are evenly scaled from 0 (dark) and 0.05 (light). All p-values above
#'   0.05 are color white. Note, this parameter also causes the graph to display
#'   p-values instead of R-squared values when add_labels is set to TRUE. FALSE
#'   (default) - Color and label plot using R-squared values. TRUE - Color and
#'   label plot using p-values.
#' @param apply_bonferroni boolean indicating whether or not to apply a
#'   Bonferroni correction to the p-value color scale. Note, this parameter has
#'   no effect if color_by_pvalue is set to FALSE. FALSE (default) - p-value
#'   colors are scaled from 0 to 0.05. All other p-values are colored white.
#'   TRUE - p-value colors are scaled from 0 to 0.05 / n, where n is the total
#'   number of non-reciprocal comparisons (e.g. "SMS.Count x Call.Count" and
#'   "Call.Count x SMS.Count" are reciprocal comparisons) performed.
#' @param showLegend boolean indicating whether or not to display legend.
#'
#' @return A ggplot object.
PlotR2s <- function(input_data,
                    title="",
                    variable_ordering=NULL,
                    triangle_heatmap=FALSE,
                    add_labels=FALSE,
                    color_by_pvalue=FALSE,
                    apply_bonferroni=FALSE,
                    showLegend=FALSE) {
    
    plot.matrix <- getR2matrix(input_data, color_by_pvalue)
    
    #User provided an ordering for the variables in this graph
    if(!is.null(variable_ordering)) {
        
        f1 <- factor(plot.matrix$factor.1, variable_ordering)
        f2 <- factor(plot.matrix$factor.2, variable_ordering)
        
        #Update factors with new ordering
        plot.matrix$factor.1 = f1
        plot.matrix$factor.2 = f2
    }
    
    if(triangle_heatmap) {
        #Remove data from lower triangle
        plot.matrix <- dcast(plot.matrix, formula = factor.1 ~ factor.2, value.var = "variability.explained")
        #Exclude factor.1 labels when determining triangle plot
        factor1 = plot.matrix$factor.1
        plot.matrix$factor.1 <- NULL
        plot.matrix[lower.tri(plot.matrix)] <- NA
        plot.matrix$factor.1 = factor1
        plot.matrix <- melt(plot.matrix,  variable.name = "factor.2",
                            value.name = "variability.explained",
                            id.vars = c("factor.1"))
        plot.matrix = na.omit(plot.matrix)
    }
    
    # generate geom_tile plot
    plot <-
        ggplot(data = plot.matrix,
               aes(x = factor.1, y = factor.2, fill = variability.explained)) +
        geom_tile(color = "black")
    
    # Adjust color scale for displaying either R-squared values or p-values.
    if(color_by_pvalue) { #Color by p-values
        
        if(apply_bonferroni) { #Apply Bonferroni correction to upper-limit of color scale
            
            #NOTE: This calculation works when the two factors we're comparing are
            #      identical. However, if there are elements in one factor not present
            #      in the other, this cutoff will be higher than the true Bonferroni-
            #      corrected value. There is room to improve this calculation to
            #      account for this possibility. However, since we're currently only
            #      using this code to plot matrices mirrored down the diagonal, this
            #      will suffice.
            bf_cutoff = 0.05 / (length(unique(plot.matrix$factor.1)) / 2)
            
            plot <- plot + scale_fill_gradientn(name="p-value",
                                                colors = c("darkred","white","white"),
                                                values = c(0,bf_cutoff,1))
            
        } else { #Scale colors between 0 and 0.05
            plot <- plot + scale_fill_gradientn(name="p-value",
                                                colors = c("darkred","white","white"),
                                                values = c(0,0.05,1))
        }
        
    } else { #Color by R-squared
        plot <- plot + scale_fill_gradientn(name="variability\nexplained",
                                            colors = c("white", "steelblue4"),
                                            values = c(0,1))
    }
    
    plot <-
        plot +
        theme(axis.text.x = element_text(angle = 90, size = 8,
                                         hjust = 1, vjust = 0.5),
              axis.text.y = element_text(size = 8),
              axis.ticks = element_blank(),
              panel.background = element_blank(),
              plot.title = element_text(hjust=0.5)) +
        ggtitle(title) +
        xlab("Factor 1") + ylab("Factor 2")
    
    if(add_labels) {
        plot <- plot + geom_text(aes(label=sprintf("%0.2f", round(variability.explained, digits = 2))),
                                 size = 1.75)
    }
    
    if(!showLegend) {
        plot <- plot + theme(legend.position = "none")
    }
    
    return(plot)
}
```


### Run VE analyses on individual subjects {#ve-analysis-individual}

First, we will perform a variance correlation analysis between all the zephyr
and/or kubios variables for each subject individually. This gives us information
about the relationships between the variables within each subject, which we will
use later for hierarchical clustering.

We'll start with the variables just from the Kubios results. Note, this code will
generate "essentially perfect fit" warnings when comparing a variable to itself
(the *getR2matrix* function compares *all* pairwise combinations of variables,
including self-self comparisons).
<!-- The above code blocks were hidden. This seems to propagate to all of the
     following code blocks. Set class.source="fold-show" to make all the
     successive code blocks visible. -->
```{r ve_analysis_by_subject_just_kubios, eval=FALSE, class.source="fold-show"}
kubios.var_explained_by_subject = tibble()

for(subjectID in unique(filtered.merged_kubios_results.for_VE$SubjectID)) {
    kubios.var_explained_by_subject = 
        kubios.var_explained_by_subject %>% 
        bind_rows(getR2matrix(filtered.merged_kubios_results.for_VE %>%
                                  filter(SubjectID == subjectID) %>% 
                                  select(-SubjectID:-Time_of_day)) %>% 
                      mutate(SubjectID = subjectID) %>% 
                      select(SubjectID, everything()))
}
```

Now we repeat the VE analyses for the Zephyr data, and the merged table of
Zephyr and Kubios data.
```{r ve_analysis_by_subject_just_zephyr, eval=FALSE}
zephyr.var_explained_by_subject = tibble()

for(subjectID in unique(merged_zephyr_data.for_VE$SubjectID)) {
    zephyr.var_explained_by_subject = 
        zephyr.var_explained_by_subject %>% 
        bind_rows(getR2matrix(merged_zephyr_data.for_VE %>%
                                  filter(SubjectID == subjectID) %>% 
                                  select(-SubjectID:-Time_of_day)) %>% 
                      mutate(SubjectID = subjectID) %>% 
                      select(SubjectID, everything()))
}
```
```{r ve_analysis_by_subject_zephyr_and_kubios, warning=FALSE, eval=FALSE}
zephyr_and_kubios.var_explained_by_subject = tibble()

for(subjectID in unique(merged_zephyr_and_kubios_data.by_hour.for_VE$SubjectID)) {
    zephyr_and_kubios.var_explained_by_subject =
        zephyr_and_kubios.var_explained_by_subject %>%
        bind_rows(getR2matrix(merged_zephyr_and_kubios_data.by_hour.for_VE %>%
                                  filter(SubjectID == subjectID) %>% 
                                  select(-SubjectID:-Time_of_day)) %>%
                      mutate(SubjectID = subjectID) %>%
                      select(SubjectID, everything()))
}
```

Lastly, save the results to a tab-delimited text file.
```{r save_all_by_subject_ve_analysis_results, eval=FALSE}
write_tsv(kubios.var_explained_by_subject,
          file.path(data_dir, "PROCESSED_DATA/VE_RESULTS/Variance_explained_results.Kubios.by_subject.txt"))
write_tsv(zephyr.var_explained_by_subject,
          file.path(data_dir, "PROCESSED_DATA/VE_RESULTS/Variance_explained_results.Zephyr.by_subject.txt"))
write_tsv(zephyr_and_kubios.var_explained_by_subject,
          file.path(data_dir, "PROCESSED_DATA/VE_RESULTS/Variance_explained_results.Zephyr_and_Kubios.by_subject.txt"))
```
<!-- Load the previously-generated VE results. This speeds up knitting, since
     it avoids re-running the above code each time to re-generate the results. -->
```{r load_all_by_subject_ve_analysis_results, echo=FALSE}
kubios.var_explained_by_subject =
    read_tsv(file.path(data_dir, "PROCESSED_DATA/VE_RESULTS/Variance_explained_results.Kubios.by_subject.txt"),
             col_types = 
                 cols(
                     SubjectID = col_character(),
                     factor.1 = col_character(),
                     factor.2 = col_character(),
                     variability.explained = col_double()
                 ))
zephyr.var_explained_by_subject =
    read_tsv(file.path(data_dir, "PROCESSED_DATA/VE_RESULTS/Variance_explained_results.Zephyr.by_subject.txt"),
             col_types = 
                 cols(
                     SubjectID = col_character(),
                     factor.1 = col_character(),
                     factor.2 = col_character(),
                     variability.explained = col_double()
                 ))
zephyr_and_kubios.var_explained_by_subject =
    read_tsv(file.path(data_dir, "PROCESSED_DATA/VE_RESULTS/Variance_explained_results.Zephyr_and_Kubios.by_subject.txt"),
             col_types = 
                 cols(
                     SubjectID = col_character(),
                     factor.1 = col_character(),
                     factor.2 = col_character(),
                     variability.explained = col_double()
                 ))
```


### Run VE analyses by study cohort {#run_ve_by_cohort}

Next, we will perform VE analyses using all subjects within each cohort. As
before, we'll repeat each analysis using just the kubios data, just the zephyr
data, and the merged zephyr/kubios data. In this case, we are excluding two
subjects (one from the 'CKD' cohort and one from the 'CKD/T2DM' cohort) are
missing the majority of their data. We will exclude these subjects from these
cohort-level analyses.
```{r load_ve_exclusion_list}
# Load the list of SubjectIDs for the subjects we're excluding
exclusion_list.ve =
    read_tsv(file.path(data_dir, "RAW_DATA/VE_SubjectID_Exclude_List.txt"),
             col_types = "c") %>% 
    pull(SubjectID)
```
```{r ve_analysis_by_cohort_just_kubios, eval=FALSE}
kubios.var_explained_by_cohort =
    filtered.merged_kubios_results.for_VE %>% 
    # Exclude subjects missing most of their data
    filter(!(SubjectID %in% exclusion_list.ve)) %>% 
    select(-SubjectID, -TimeSubjectIndex:-Time_of_day) %>% 
    nest(Data = -Cohort) %>% 
    mutate(VE_results = purrr::map(Data, getR2matrix)) %>% 
    select(-Data) %>% 
    unnest(VE_results)
```
```{r ve_analysis_by_cohort_just_zephyr, eval=FALSE}
zephyr.var_explained_by_cohort =
    merged_zephyr_data.for_VE %>% 
    # Exclude subjects missing most of their data
    filter(!(SubjectID %in% exclusion_list.ve)) %>% 
    select(-SubjectID, -TimeSubjectIndex:-Time_of_day) %>% 
    nest(Data = -Cohort) %>% 
    mutate(VE_results = purrr::map(Data, getR2matrix)) %>% 
    select(-Data) %>% 
    unnest(VE_results)
```
```{r ve_analysis_by_cohort_zephyr_and_kubios, eval=FALSE}
zephyr_and_kubios.var_explained_by_cohort =
    merged_zephyr_and_kubios_data.by_hour.for_VE %>% 
    # Exclude subjects missing most of their data
    filter(!(SubjectID %in% exclusion_list.ve)) %>% 
    select(-SubjectID, -TimeSubjectIndex:-Time_of_day) %>% 
    nest(Data = -Cohort) %>% 
    mutate(VE_results = purrr::map(Data, getR2matrix)) %>% 
    select(-Data) %>% 
    unnest(VE_results)
```

Save the results of the VE analyses to tab-delimited files.
```{r save_all_by_cohort_ve_analysis_results, eval=FALSE}
write_tsv(kubios.var_explained_by_cohort,
          file.path(data_dir, "PROCESSED_DATA/VE_RESULTS/Variance_explained_results.Kubios.by_cohort.txt"))
write_tsv(zephyr.var_explained_by_cohort,
          file.path(data_dir, "PROCESSED_DATA/VE_RESULTS/Variance_explained_results.Zephyr.by_cohort.txt"))
write_tsv(zephyr_and_kubios.var_explained_by_cohort,
          file.path(data_dir, "PROCESSED_DATA/VE_RESULTS/Variance_explained_results.Zephyr_and_Kubios.by_cohort.txt"))
```
<!-- Load the previously-generated VE results. This speeds up knitting, since
     it avoids re-running the above code each time to re-generate the results. -->
```{r load_all_by_cohort_ve_analysis_results, echo=FALSE}
kubios.var_explained_by_cohort =
    read_tsv(file.path(data_dir, "PROCESSED_DATA/VE_RESULTS/Variance_explained_results.Kubios.by_cohort.txt"),
             col_types =
                 cols(.default = col_character(),
                      Cohort = readr::col_factor(levels = cohort_levels),
                      variability.explained = col_double()))
zephyr.var_explained_by_cohort =
    read_tsv(file.path(data_dir, "PROCESSED_DATA/VE_RESULTS/Variance_explained_results.Zephyr.by_cohort.txt"),
             col_types =
                 cols(.default = col_character(),
                      Cohort = readr::col_factor(levels = cohort_levels),
                      variability.explained = col_double()))
zephyr_and_kubios.var_explained_by_cohort =
    read_tsv(file.path(data_dir, "PROCESSED_DATA/VE_RESULTS/Variance_explained_results.Zephyr_and_Kubios.by_cohort.txt"),
             col_types =
                 cols(.default = col_character(),
                      Cohort = readr::col_factor(levels = cohort_levels),
                      variability.explained = col_double()))
```




# Two-Way ANOVA - Time of Day x Study Cohort {#two-way-anova-analysis}

We want to identify which Zephyr and Kubios metrics show differences between
cohort (CKD, CKD/T2DM, Healthy controls) and/or time of day (day, night). This
lends itself naturally to a two-way ANOVA using time of day and study cohort as
our two factors of interest. We will perform a two-way ANOVA analysis on each
Kubios and Zephyr metric, followed by a multiple testing correction across all
tests.


## Two-way ANOVA Analyses

For each subject, we will calculate the mean measurement during the day
(07:00-21:59) and the night (22:00 - 06:59) for each Zephyr and Kubios variable.
We will then analyze these mean values using a two-way ANOVA model that includes
and interaction term for time of day and cohort. For the time of day analyses,
we will be comparing data from the same subjects during the day and night,
making this a repeated measure test for the time of day factor. To account for
the repeated measure, we will introduce the subject ID into our ANOVA model as
an error term for the time of day comparison.

Note, for these analyses, we do not need to perform any significant
pre-processing or formatting of the data, as we added time of day and cohort
information to the data tables [above](#add-tod-and-cohort-columns). We are
excluding the same subjects we excluded from the VE
[analyses](#run_ve_by_cohort), because they are missing data over the majority
of their measurement periods.
```{r two_way_anova_analysis_kubios, eval=FALSE}
# There is randomness involved in the model fitting. Set random seed for
# reproducibility.
set.seed(42)

kubios.two_way_anova_results =
    filtered.merged_kubios_results.for_VE %>% 
    # Exclude subjects missing most of their data
    filter(!(SubjectID %in% exclusion_list.ve)) %>%
    pivot_longer(cols=c(-SubjectID:-Time_of_day), names_to="Measurement", values_to="Value") %>% 
    group_by(SubjectID, Cohort, Time_of_day, Measurement) %>% 
    summarise(Mean = mean(Value, na.rm = TRUE),
              SD = sd(Value, na.rm = TRUE),
              Total_measurements= sum(!is.na(Value)),
              .groups = "drop") %>% 
    # nest_by returns a rowwise tibble, so the mutate is performed separately on
    # each Measurement.
    nest_by(Measurement) %>% 
    # We're leaving the anova model as a list column, so we can both summarize
    # the ANOVA results, and also use them as input for post-hoc testing below.
    mutate(anova_model = list(nlme::lme(Mean ~ Cohort * Time_of_day,
                                        random = ~ 1|SubjectID/Time_of_day, data=data)))
```

For the Zephyr data, one subject is missing all HRV data for the night, meaning
we cannot perform a day/night comparison for this subject. Here, we remove all
HRV data from this subject before performing the two-way ANOVA.
<!-- Break this into a separate chunk so it can run independent of the ANOVA
     code chunks below. This way downstream chunks can make use of the ANOVA
     exclusion list without need to re-run the Zephyr ANOVA.-->
```{r two_way_anova_analysis_zephyr.get_id_for_excluded_subject}
# Load ID of subject we're excluding from the HRV ANOVA analysis
exclusion_list.anova = 
    read_tsv(file.path(data_dir, "RAW_DATA/ANOVA_SubjectID_Exclude_List.txt"),
             col_types = "c") %>% 
    pull(SubjectID)
```
```{r two_way_anova_analysis_zephyr, eval=FALSE}
# There is randomness involved in the model fitting. Set random seed for
# reproducibility.
set.seed(42)

zephyr.two_way_anova_results =
    merged_zephyr_data.for_VE %>% 
    # Exclude subjects missing most of their data
    filter(!(SubjectID %in% exclusion_list.ve)) %>%
    pivot_longer(cols=c(-SubjectID:-Time_of_day), names_to="Measurement", values_to="Value") %>% 
    group_by(SubjectID, Cohort, Time_of_day, Measurement) %>% 
    summarise(Mean = mean(Value, na.rm = TRUE),
              SD = sd(Value, na.rm = TRUE),
              Total_measurements= sum(!is.na(Value)),
              .groups = "drop") %>% 
    # Remove HRV data for subject missing all data at night
    filter(!(Measurement == "HRV" & SubjectID %in% exclusion_list.anova)) %>%
    # nest_by returns a rowwise tibble, so the mutate is performed separately on
    # each Measurement.
    nest_by(Measurement) %>%
    # We're leaving the anova model as a list column, so we can both summarize
    # the ANOVA results, and also use them as input for post-hoc testing below.
    mutate(anova_model = list(nlme::lme(Mean ~ Cohort * Time_of_day,
                                        random = ~ 1|SubjectID/Time_of_day, data=data)))
```


### Multiple Testing Correction

Above we performed a two-way ANOVA on each Zephyr/Kubios measurement. This
amounts to 103 separate ANOVA tests, each of which generate three p-values
(cohort, time of day, interaction). In order to avoid a multiple testing
problem, we are going to apply a Benjamini-Hochberg FDR correction  across all
3 x 103 = 309 of these p-values.
```{r two_way_anova_fdr_correction, eval=FALSE}
zephyr_and_kubios.two_way_anova_results.w_fdr =
    # Start by extracting the summarized Zephyr and Kubios ANOVA results, then
    # merging them together.
    bind_rows(zephyr.two_way_anova_results %>% 
                  # Use the anova() function to extract a summary of the ANOVA
                  # results from the nlme object. The anova() function returns
                  # an object where the ANOVA factor/term stored as the column
                  # names. To extract this info with the rownames_to_column()
                  # function, convert to data.frame first.
                  summarise(anova(anova_model) %>%
                                as.data.frame() %>%
                                rownames_to_column(var = "term") %>%
                                as_tibble(),
                            .groups = "drop") %>% 
                  mutate(Data_Source = "Zephyr"),
              kubios.two_way_anova_results %>% 
                  summarise(anova(anova_model) %>%
                                as.data.frame() %>%
                                rownames_to_column(var = "term") %>%
                                as_tibble(),
                            .groups = "drop") %>% 
                  mutate(Data_Source = "Zephyr") %>% 
                  mutate(Data_Source = "Kubios")) %>% 
    select(Measurement, Data_Source, everything(), -denDF) %>% 
    filter(term != "(Intercept)") %>%
    rename(ANOVA.p_value = `p-value`,
           ANOVA.df = numDF,
           ANOVA.f_statistic = `F-value`) %>%
    mutate(ANOVA.q_value = p.adjust(ANOVA.p_value, method = "BH")) %>%
    select(Measurement, Data_Source, term, starts_with("ANOVA")) %>%
    # Determine minimum q-value for each measurement, and use this to sort
    # the table.
    group_by(Data_Source, Measurement) %>%
    mutate(min_q = min(ANOVA.p_value, na.rm = TRUE)) %>%
    ungroup() %>%
    # Order term for display purposes
    mutate(term = factor(term, levels = c("Time_of_day", "Cohort", "Cohort:Time_of_day"))) %>%
    arrange(min_q, Measurement, term) %>%
    select(-min_q)
```

Save the results of the two-way ANOVA analyses to a tab-delimited file.
```{r save_two_way_anova_results, eval=FALSE}
write_tsv(zephyr_and_kubios.two_way_anova_results.w_fdr,
          file.path(data_dir, "PROCESSED_DATA/ANOVA_RESULTS/Two_Way_ANOVA_results.TimeOfDay_v_Cohort.Zephyr_and_Kubios2.txt"))
```
<!-- Load the previously-generated ANOVA results. This speeds up knitting, since
     it avoids re-running the above code each time. -->
```{r load_two_way_anova_results, echo=FALSE}
zephyr_and_kubios.two_way_anova_results.w_fdr =
    read_tsv(file.path(data_dir, "PROCESSED_DATA/ANOVA_RESULTS/Two_Way_ANOVA_results.TimeOfDay_v_Cohort.Zephyr_and_Kubios.txt"),
             col_types =
                 cols(
                     Measurement = col_character(),
                     Data_Source = col_character(),
                     term = readr::col_factor(levels = c("Time_of_day", "Cohort", "Cohort:Time_of_day")),
                     ANOVA.df = col_integer(),
                     ANOVA.f_statistic = col_double(),
                     ANOVA.p_value = col_double(),
                     ANOVA.q_value = col_double()
                 ))

# Load ID of subject we're excluding from the HRV ANOVA analysis. This is used
# below and is not loaded if the chunks above are not run.
exclusion_list.anova = 
    read_tsv(file.path(data_dir, "RAW_DATA/ANOVA_SubjectID_Exclude_List.txt"),
             col_types = "c") %>% 
    pull(SubjectID)
```



## Post-Hoc Testing

The two-way ANOVA analyses tested for significant differences in Kubios/Zephyr
measurements across cohort, time of day, or their interaction. For time of day,
this result is very east to interpret: a significant ANOVA q-value indicates
there is a significant difference between measurements take during the night and
day periods. For cohort, the interpretation is less clear. A significant ANOVA
q-value for cohort indicates that there are significant differences at the
cohort level, but does not identify which of the three cohorts (CKD, CKD/T2DM,
Healthy controls) are driving the significant differences. The same problem
holds true for the interactions.

In order to identify which specific cohorts are driving the significant ANOVA
results, we need to perform *post-hoc* testing. By using Tukey's HSD (honestly
significant difference) test, we are effectively performing all possible
pairwise comparisons, while controlling for multiple testing.

Note, while we are performing *post-hoc* test on all of the Kubios/Zephyr
measurements for convenience, in our final analyses we only consider the results
for those measurements that also have a significant q-value from the two-way
ANOVA analyses. This is important, given that the two-way ANOVA q-values are
controlled for multiple testing across all of the measurements, while the
*post-hoc* tests only control for multiple testing within a given measurement.
```{r two_way_anova_post_hoc, eval=FALSE, warning=FALSE, message=FALSE}
# There is randomness involved in the model fitting and post-hoc calculation.
# Set random seed for reproducibility.
set.seed(42)

zephyr_and_kubios.two_way_anova_results.posthoc =
    # Data are still nested by Measurement. This is intentional.
    bind_rows(zephyr.two_way_anova_results %>% 
                  mutate(Data_Source = "Zephyr"),
              kubios.two_way_anova_results %>% 
                  mutate(Data_Source = "Kubios")) %>% 
    mutate(post_hoc_results.cohort =
               list(summary(multcomp::glht(anova_model, linfct=multcomp::mcp(Cohort="Tukey"))) %>% 
                        broom::tidy() %>% 
                        mutate(Effect = factor("Main", levels = c("Main", "Interaction")))),
           # This model is equivalent to the 'Mean ~ Cohort * Time_of_day' model
           # used above, but it's easier to extract the Interaction contrasts
           # from this model.
           anova_model.interaction =
               list(data %>%
                        mutate(Interaction_term = interaction(Cohort, Time_of_day, sep=":")) %>% 
                        nlme::lme(Mean ~ Interaction_term, random = ~ 1|SubjectID/Time_of_day, data=.)),
           post_hoc_results.interaction =
               list(summary(multcomp::glht(anova_model.interaction, linfct=multcomp::mcp(Interaction_term="Tukey"))) %>% 
                        broom::tidy() %>% 
                        mutate(Effect = factor("Interaction", levels = c("Main", "Interaction"))))
           ) %>%
    summarize(Data_Source, bind_rows(post_hoc_results.cohort, post_hoc_results.interaction),
              .groups = "drop") %>% 
    rename(Contrast = contrast,
           Adj.p_value = adj.p.value) %>% 
    select(Measurement, Data_Source, Effect, everything()) %>% 
    # Update contrast names for readability and exclude contrasts that do not make sense
    separate(Contrast, into = c("Cont_1", "Cont_2"), sep = " - ", remove = FALSE) %>% 
    separate(Cont_1, into = c("Cont_1.Cohort", "Cont_1.Time_of_day"), sep = ":", fill = "right") %>% 
    separate(Cont_2, into = c("Cont_2.Cohort", "Cont_2.Time_of_day"), sep = ":", fill = "right") %>% 
    # Note, I'm adding a "1-" or "2-" prefix to the Updated_Contrast, because I
    # want to re-order so the within-Cohort contrasts are grouped together
    # first, followed by the within-Time_of_Day contrasts.
    mutate(Updated_Contrast = 
               case_when(
                   Effect == "Main" ~ paste0(Cont_1.Cohort, " - ", Cont_2.Cohort),
                   Cont_1.Cohort == Cont_2.Cohort ~ paste0("1-", Cont_1.Cohort, ":(", Cont_1.Time_of_day, " - ", Cont_2.Time_of_day, ")"),
                   Cont_1.Time_of_day == Cont_2.Time_of_day ~ paste0("2-", Cont_1.Time_of_day, ":(", Cont_1.Cohort, " - ", Cont_2.Cohort, ")"),
                   # For our purposes, the completely-crossed contrasts are not
                   # interesting and difficult to interpret. I'm keeping them here
                   # for the sake of completeness.
                   TRUE ~ Contrast
               )) %>% 
    arrange(Measurement, Effect, Updated_Contrast) %>% 
    # Remove ordering prefix
    mutate(Updated_Contrast = stringr::str_remove(Updated_Contrast, "^[[:digit:]]-")) %>% 
    select(Measurement, Effect, Updated_Contrast, tidyr::everything(),
           -tidyr::starts_with("Cont")) %>%
    rename(Contrast = Updated_Contrast)
```

Save the results of the *post-hoc* tests to a tab-delimited file.
```{r save_two_way_anova_post_hoc, eval=FALSE}
write_tsv(zephyr_and_kubios.two_way_anova_results.posthoc,
          file.path(data_dir, "PROCESSED_DATA/ANOVA_RESULTS/Two_Way_ANOVA_PostHoc_results.TimeOfDay_v_Cohort.Zephyr_and_Kubios.txt"))
```
<!-- Load the previously-generated post-hoc results. This speeds up knitting, since
     it avoids re-running the above code each time. -->
```{r load_two_way_anova_post_hoc, echo=FALSE}
zephyr_and_kubios.two_way_anova_results.posthoc =
    read_tsv(file.path(data_dir, "PROCESSED_DATA/ANOVA_RESULTS/Two_Way_ANOVA_PostHoc_results.TimeOfDay_v_Cohort.Zephyr_and_Kubios.txt"),
             col_types =
                 cols(
                     Measurement = col_character(),
                     Effect = readr::col_factor(levels = c("Main", "Interaction")),
                     Contrast = col_character(),
                     Data_Source = col_character(),
                     estimate = col_double(),
                     std.error = col_double(),
                     statistic = col_double(),
                     Adj.p_value = col_double()
                 ))
```

# Cosinor Analyses {#cosinor-analyses}

Since we have data collected continuously over two days, these data are ideal
for characterizing circadian oscillations in the various Zephyr and Kubios
metrics. We do this by fitting cosine curves with 24-hour periods to each metric,
a method generally called *cosinor analysis*. Using the fitted cosine curves, we
can derive the key oscillatory parameters of any circadian patterns:

* MESOR
    + Rhythm-adjust mean across the entire oscillatory pattern.
    + Reflects he baseline value for a metric.
* Amplitude
    + Distance from the MESOR to the peak value. 
    + Reflects the extent to which the metric varies over time.
* Phase or Acrophase
    + Time at which the cosine curve reaches its first peak value.
    + Useful to compare the relative timing of patterns between different
    metrics, cohorts, or individuals.

![Figure 1 from Cornelissen, 2014.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3991883/bin/1742-4682-11-16-1.jpg)

The methods we used for the cosinor analyses are adapted from the single-component
Cosinor analysis reviewed by [Cornelissen, 2014](https://pubmed.ncbi.nlm.nih.gov/24725531/),
as well as the cosine fit described by [Refinetti et al., 2007](https://pubmed.ncbi.nlm.nih.gov/23710111/).

To accurately estimate these parameters from a time course, we need a dense
sampling of the data. The Zephyr data are ideal, given they were aggregated at 1
minute intervals. The Kubios data were generated at hourly intervals, some of
which were dropped during quality filtering. This sparsity makes the Kubios data
less suitable for cosinor analyses, so we focus exclusively on the Zephyr data.

## Functions for Cosinor Analyses

We performed the cosinor fits of the Zephyr data during [pre-processing](#zephyr-preprocess).
Specifically, the [pre-processing script](./R/process_and_plot_zephyr_data_by_subject.R)
uses the following function to perform the cosinor fits.
```{r define_cosinor_fit_function, eval=FALSE}
#' Cosinor analysis
#'
#' The following algorithm is adapted from
#' https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3991883/ . This function requires
#' the rlang, broom, purr, dplyr, tidyr, and readr packages.
#'
#' @param input_data A tbl() or data.frame containing time and signal columns
#'   for cosinor analysis.
#' @param time Bare name of the column with the time information. The time
#'   column should be a type compatible with the difftime() function.
#' @param signal Bare name of the column with the signal data. The signal column
#'   should be of type dbl.
#' @param ... (Optional) Bare column names to group by.
#' @param period Length of period for cosinor analysis in hours (Default: 24).
#' @param confidence_level The level of confidence used when determining the
#'   confidence interval (Default: 0.95).
#'
#' @return
#' @export
#'
#' @examples
fit_cosine_to_data <- function(input_data,
                               time,
                               signal,
                               ...,
                               period=24,
                               confidence_level=0.95) {
    
    # "Quote" column names so they are ready for use below
    time_col <- enquo(time)
    signal_col <- enquo(signal)
    group_by_cols <- quos(...)
    
    # Perform regression based on the given grouping variables.
    regression_results =
        input_data %>% 
        # Renaming the signal column simplifies the code below. I could unquote
        # all occurrences of this column using UQ(), like I do with the time
        # column. However, this doesn't work in the lm() call below.
        rename(Signal_Column = UQ(signal_col)) %>% 
        #Calculate the start times for each signal.
        group_by(UQS(group_by_cols)) %>% 
        mutate(Group.Start = min(UQ(time_col))) %>% 
        ungroup() %>% 
        # Calculate the Cosinor.Time as hours since the start time. The
        # Corrected.Cosinor.Time converts the hours since the start time to match
        # the 24-hour clock time. This makes it easier to calculate the phase
        # below.
        mutate(Cosinor.Time = as.double(difftime(UQ(time_col), Group.Start, units=c("hours"))),
               Corrected.Cosinor.Time = Cosinor.Time + hour(Group.Start) +
                   minute(Group.Start)/60 + second(Group.Start)/3600) %>% 
        select(UQS(group_by_cols), Corrected.Cosinor.Time, Signal_Column) %>% 
        unique() %>% 
        group_by(UQS(group_by_cols)) %>% 
        mutate(x = cos(2*pi*Corrected.Cosinor.Time/period),
               z = sin(2*pi*Corrected.Cosinor.Time/period)) %>% 
        tidyr::nest() %>% 
        # Perform fit and extract results at the coefficient level and fit level.
        mutate(fit = purrr::map(data, ~ lm(Signal_Column ~ x + z, data=.x)),
               # Return the 95% confidence intervals for each coefficient.
               tidied = purrr::map(fit, broom::tidy, conf.int=TRUE, conf.level=confidence_level),
               glanced = purrr::map(fit, broom::glance))
    
    # Merge the coefficient data with the fit summary stats. Note, I had to use
    # rbind instead of bind_rows, because this was causing some weird conflicts
    # with the UQS quosures that I couldn't debug.
    rbind(
        regression_results %>%
            select(-data, -fit, -glanced) %>% 
            tidyr::unnest(tidied) %>%
            # Now rename coefficients to match what names we use to derive cosinor parameters
            mutate(term = recode(term,
                                 `(Intercept)` = "MESOR",
                                 `x` = "beta",
                                 `z` = "gamma")) %>%
            select(-statistic) %>%
            gather(stat, value, -c(UQS(group_by_cols)), -term) %>%
            unite(lm.results, term, stat, sep="_"),
        regression_results %>%
            select(-data, -fit, -tidied) %>%
            tidyr::unnest(glanced) %>%
            gather(term, value, -c(UQS(group_by_cols))) %>%
            mutate(lm.results = paste0("fit_", term)) %>%
            select(UQS(group_by_cols), lm.results, value)
    ) %>%
        arrange(UQS(group_by_cols), lm.results)
}
```

This function is fitting the following linear regression formula to the data:
$$Y(t) = M + \beta x + \gamma z + e(t)$$
where 
$$x = cos(2\pi t/\tau ),$$
$$\mathrm{and} ~ z = sin(2\pi t/\tau ).$$
Also, $t$ is the time variable in hours, $Y(t)$ is the vector of $Y_i$ values
for a given Zephyr/Kubios metric measured at a specific time $t_i$, $e(t)$ is
the vector of error/noise values at each time, and $\tau$ is the period in
hours, which we are fixing at 24 since we want to extract the characteristics of
circadian oscillations. A linear regression on these equations yields estimates
for the $\beta$ and $\gamma$ parameters. These parameters have the following
identities:

$$\beta = Acos, $$
$$\mathrm{and}~\gamma = ~Asin\phi$$
where, $A$ is the amplitude, $\phi$ is the acrophase. So using these identities,
along with the $\hat\beta$ and $\hat\gamma$ estimates, we can estimates the
the amplitude with the following formula:
$$\hat A = \sqrt{\hat\beta^2 +~ \hat\gamma^2}$$
The calculation for the $\hat\phi$ estimate is a little more complex, as its
value changes, depending upon the signs of $\hat\beta$ and $\hat\gamma$. We use
the following formulas from [Refinetti et al., 2007](https://pubmed.ncbi.nlm.nih.gov/23710111/)
to calculate $\hat\phi$:
$$
\begin{eqnarray}
\hat\phi & = & -arctan \left\lvert \frac{\hat\gamma}{\hat\beta} \right\rvert & ~~ \mathrm{if} ~ \hat\gamma > 0 ~ \mathrm{and} ~ \hat\beta \ge 0 \\
& = & -\pi +  arctan \left\lvert \frac{\hat\gamma}{\hat\beta} \right\rvert & ~~ \mathrm{if} ~ \hat\gamma > 0 ~ \mathrm{and} ~ \hat\beta < 0 \\
& = & -\pi - arctan \left\lvert \frac{\hat\gamma}{\hat\beta} \right\rvert & ~~ \mathrm{if} ~ \hat\gamma \le 0 ~ \mathrm{and} ~ \hat\beta \le 0 \\
& = & -2\pi +  arctan \left\lvert \frac{\hat\gamma}{\hat\beta} \right\rvert & ~~ \mathrm{if} ~ \hat\gamma \le 0 ~ \mathrm{and} ~ \hat\beta > 0 \\
\end{eqnarray}
$$

At this point, $\hat\phi$ is an estimate of the acrophase in radians. We need to
convert this into an hour on the 0-23 hour scale in order to make the phase more
readily interpretable.
$$Phase = -\phi (\frac{\tau}{2\pi}) ~~ \mathrm{mod} ~~ \tau$$
Again, $\tau$ is fixed at 24 hours since we're focused on circadian
oscillations. Since phase is a circular measure (i.e. a phase of 27 corresponds
to the same clock hour as a phase of 3 [27 = 24 + 3]) the modulus operation maps
an values that exceed 23 back to the 0-23 hour scale.

Lastly, the MESOR is the model's intercept term.

We implement these formulas in the following function, which takes the
$\hat\beta$ and $\hat\gamma$ estimates from the *fit_cosine_to_data* function we
defined above.
```{r define_cosinor_param_function}
#' Cosinor analysis
#'
#' Accepts results generated by the fit_cosine_to_data() function and calculates
#' the amplitude, phi, and phase cosine parameters.
#'
#' @param input_data A tbl() or data.frame generated by the fit_cosine_to_data()
#'   function.
#' @param period Length of period (in hours) used in the cosinor analysis being
#'   processed (Default: 24).
#'
#' @return
#' @export
#'
#' @examples
get_circadian_params_from_cosine_fit_results <- function(input_data,
                                                         period=24) {
    
    input_data %>% 
        filter(grepl("_estimate", lm.results)) %>% 
        mutate(lm.results = gsub("_estimate", "", lm.results)) %>% 
        spread(lm.results, value) %>% 
        mutate(amp = sqrt(beta^2 + gamma^2),
               pre.phi = atan(abs(gamma/beta)),
               #phi depends upon signs of beta and gamma
               phi = ifelse(gamma > 0,
                            ifelse(beta >= 0, -pre.phi, -pi + pre.phi),
                            ifelse(beta <= 0, -pi - pre.phi, -2*pi + pre.phi)),
               phase = ((period / (2*pi)) * (-phi)) %% period) %>% 
        #Add stats from the fit
        left_join(input_data %>% 
                      filter(lm.results %in% c("fit_p.value", "fit_r.squared", "fit_adj.r.squared", "fit_sigma")) %>%
                      mutate(lm.results = factor(lm.results, levels=c("fit_p.value", "fit_r.squared", "fit_adj.r.squared", "fit_sigma"))) %>% 
                      spread(lm.results, value))
}
```


## Merge Cosinor Fit Results and Estimate Circadian Parameters

We performed all of the Zephyr cosinor fits as part of the Zephyr pre-processing
steps. For each of downstream analyses, we want to merge all zephyr metrics
across all subjects into a single table, and calculate the circadian parameters
MESOR, amplitude, and phase.
<!-- There is a suppressed warning here. The CRIC subjects don't have a 'Misc'
     column and will generate a lot of parser warnings. -->
```{r merge_cosinor_data, eval=FALSE}
zephyr_cosinor_files =
    list.files(path = file.path(data_dir, "PROCESSED_DATA/"),
               recursive = TRUE, full.names = TRUE,
               pattern = "Cosinor_results\\.ZephyrBioPatch\\..*\\.txt")

#Aggregate the raw Cosinor results, as well as the derived circadian stats
#(Amplitude, Phase).
merged_cosinor_results.raw_stats = tibble()
merged_cosinor_results.derived_stats = tibble()

#Read raw results, calculate derived stats, then merge these.
for(file in zephyr_cosinor_files) {
    
    #Read raw data and merge
    raw_cosinor_data =
        read_tsv(file,
                 col_types = 
                     cols(
                         SubjectID = col_character(),
                         Source = col_character(),
                         Measurement = col_character(),
                         lm.results = col_character(),
                         value = col_double(),
                         Misc = col_character()
                     )) %>% 
        # The Misc column is only used by the control subject to identify the
        # device used to collect the data (since the controls alternated between
        # using two devices). Moving forward, we don't need to maintain this
        # information.
        select(SubjectID, everything(), -any_of("Misc"))
    
    merged_cosinor_results.raw_stats = 
        merged_cosinor_results.raw_stats %>% 
        bind_rows(raw_cosinor_data)
    
    #Derive circadian stats from cosinor data
    merged_cosinor_results.derived_stats =
        merged_cosinor_results.derived_stats %>% 
        bind_rows(get_circadian_params_from_cosine_fit_results(raw_cosinor_data))
}

# Add Cohort label to each subject
merged_cosinor_results.raw_stats =
    merged_cosinor_results.raw_stats %>% 
    left_join(cric_diabetic_designation, by = "SubjectID") %>% 
    select(SubjectID, Cohort, everything())
merged_cosinor_results.derived_stats =
    merged_cosinor_results.derived_stats %>% 
    left_join(cric_diabetic_designation, by = "SubjectID") %>% 
    select(SubjectID, Cohort, everything())
```

Now save the merged cosinor results to tab-delimited text files.
```{r save_cosinor_results, eval=FALSE}
merged_cosinor_results.raw_stats %>% 
    write_tsv(file.path(data_dir, "PROCESSED_DATA/COSINOR_RESULTS/Cosinor_results.Raw.Zephyr.merged_across_subjects.txt"))

merged_cosinor_results.derived_stats %>% 
    #Remove intermediates for calculating derived stats
    select(-beta, -gamma, -pre.phi, -phi) %>% 
    write_tsv(file.path(data_dir, "PROCESSED_DATA/COSINOR_RESULTS/Cosinor_results.Derived_and_fit_stats.Zephyr.merged_across_subjects.txt"))
```
<!-- Load the previously-generated cosinor results. This speeds up knitting,
     since it avoids re-running the above code each time. -->
```{r load_cosinor_results, echo=FALSE}
merged_cosinor_results.raw_stats = 
    read_tsv(file.path(data_dir, "PROCESSED_DATA/COSINOR_RESULTS/Cosinor_results.Raw.Zephyr.merged_across_subjects.txt"),
             col_types =
                 cols(
                     .default = col_character(),
                     Cohort = readr::col_factor(levels = cohort_levels),
                     value = col_double()
                 ))

merged_cosinor_results.derived_stats =
    read_tsv(file.path(data_dir, "PROCESSED_DATA/COSINOR_RESULTS/Cosinor_results.Derived_and_fit_stats.Zephyr.merged_across_subjects.txt"),
             col_types =
                 cols(
                     SubjectID = col_character(),
                     Cohort = readr::col_factor(levels = cohort_levels),
                     Source = col_character(),
                     Measurement = col_character(),
                     .default = col_double()
                 ))
```

## LOESS-derived Amplitudes and Phases

The parameters we estimated about from a cosinor fit are only meaningful for a
given metric/subject if the data actually approximate a sinusoidal pattern. For
comparison purposes, we also used a LOESS-style approach to estimate the
amplitudes and phases of the Zephyr metrics for each subject. This alternative
approach does not require the data to fit to a cosine curve. Instead, we
collapse the data points to a single day using their clock times (0-23 hours),
and then fit a weighted, smoothed curve to the data. We then estimate the
amplitude using the minimum and maximum values on this smoothed curve, and we
estimate the phase by identifying the time at which the curve reaches its
maximum value.

We are not using this approach to identify oscillatory patterns in the data.
Rather, we're examining the phase/amplitude estimates we get when using a naive
method that does not require a cosine fit.

Note, to mitigate the edge effects of a sliding-window approach, we duplicate
the data before and after the measurement period.
```{r loess_amp_and_phase_estimates, eval=FALSE}
# There is randomness involved in the model fitting. Set random seed for
# reproducibility.
set.seed(42)

# These parameters tune the proportion of the data to include in the smoothing
# window. The weight_radius specifies the proportion of the data from a single
# day we want to include in the sliding window. Since we duplicated the data
# before and after the measurement period to mitigate edge effects, we need to
# divide the desired weight_radius by 3 to get the final span of the window for
# our LOESS fit.
weight_radius = 0.7
span = (weight_radius / 3)

# Perform LOESS fit by subject and measurement
zephyr.loess_estimates =
    merged_zephyr_data.for_VE %>% 
    # Filter out subjects missing substantial amounts of data (identified these
    # as part of the VE analyses).
    filter(!(SubjectID %in% exclusion_list.ve)) %>% 
    select(SubjectID, Cohort, ClockHour, Activity:Posture) %>%
    # select(SubjectID, Cohort, ClockHour, Activity, BR, HR, HRV) %>%
    # Duplicate data before and after current period to mitigate edge effects
    # during the LOESS fit.
    mutate(PreClockHour = ClockHour - 24,
           PostClockHour = ClockHour + 24) %>% 
    pivot_longer(cols = c(PreClockHour, ClockHour, PostClockHour),
                 names_to = "Period", values_to = "Time_with_flanks") %>% 
    pivot_longer(cols = -c(SubjectID, Cohort, Period, Time_with_flanks),
                 names_to = "Measurement", values_to = "Value") %>% 
    arrange(SubjectID, Cohort, Measurement, Time_with_flanks) %>% 
    group_by(SubjectID, Cohort, Measurement) %>% 
    nest() %>% 
    mutate(Loess_fit =
               purrr::map(data,
                          ~loess(formula = Value ~ Time_with_flanks, degree = 2,
                                 span = span,
                                 method = "loess", normalize = FALSE,
                                 family = "gaussian",
                                 data=.x)),
           Loess_by_hour =
               purrr::map(Loess_fit,
                          ~predict(.x, newdata = data.frame(Time_with_flanks=seq(0,23))) %>% 
                              as_tibble() %>% 
                              rename(Fitted_Values = value) %>% 
                              mutate(Time_with_flanks=seq(0,23),
                                     Loess.peak_time = Time_with_flanks[which.max(Fitted_Values)],
                                     Loess.trough_time = Time_with_flanks[which.min(Fitted_Values)],
                                     Loess.peak_value = max(Fitted_Values, na.rm = T),
                                     Loess.trough_value = min(Fitted_Values, na.rm = T))))

# Extract the peak/trough times and line up with corresponding cosinor stats
zephyr.loess_estimate_vs_cosinor_stats =
    zephyr.loess_estimates %>% 
    select(-data, -Loess_fit) %>% 
    unnest(Loess_by_hour) %>% 
    select(-Fitted_Values, -Time_with_flanks) %>% 
    distinct() %>% 
    mutate(Loess.amplitude = (Loess.peak_value - Loess.trough_value) / 2) %>% 
    left_join(merged_cosinor_results.derived_stats,
              by = c("SubjectID", "Cohort", "Measurement")) %>% 
    mutate(Phase.Cosinor_v_Loess = phase - Loess.peak_time,
           # Correct for phase differences > 12
           Phase.Cosinor_v_Loess =
               case_when(
                   Phase.Cosinor_v_Loess > 12 ~ (Phase.Cosinor_v_Loess %% 12) - 12,
                   Phase.Cosinor_v_Loess < -12 ~ (Phase.Cosinor_v_Loess %% -12) + 12,
                   TRUE ~ Phase.Cosinor_v_Loess
               ),
           Amp.Cosinor_v_Loess = amp - Loess.amplitude) %>% 
    rename(Cosinor.phase = phase, Cosinor.amp = amp) %>% 
    select(SubjectID, Cohort, Measurement, Loess.peak_time, Loess.trough_time,
           Loess.peak_value,  Loess.trough_time, Cosinor.phase, Phase.Cosinor_v_Loess,
           Loess.amplitude, Cosinor.amp, Amp.Cosinor_v_Loess) %>% 
    arrange(Cohort, SubjectID)
```

Now save the resulting comparison between the LOESS and cosinor estimates for
phase and amplitude to a tab-delimited text file.
```{r save_loess_cosinor_comparison, eval=FALSE}
write_tsv(zephyr.loess_estimate_vs_cosinor_stats,
          file.path(data_dir, "PROCESSED_DATA/COSINOR_RESULTS/Cosinor_vs_Loess.Phase_and_amplitude.Zephyr.Activity_HRV_HR_BP.txt"))
```

<!-- Load the previously-generated LOESS vs cosinor results. This speeds up
     knitting, since it avoids re-running the above code each time. -->
```{r load_loess_cosinor_comparison, echo=FALSE, eval=FALSE}
zephyr.loess_estimate_vs_cosinor_stats =
    read_tsv(file.path(data_dir, "PROCESSED_DATA/COSINOR_RESULTS/Cosinor_vs_Loess.Phase_and_amplitude.Zephyr.Activity_HRV_HR_BP.txt"),
             col_types =
                 cols(
                     SubjectID = col_character(),
                     Cohort = readr::col_factor(levels = cohort_levels),
                     Measurement = col_character(),
                     Loess.peak_time = col_integer(),
                     Loess.trough_time = col_integer(),
                     .default = col_double()
                 ))
```

# Hierarchical Clustering

For the VE analyses we ran [above](#ve-analysis-individual), we generated lists
of $R^2$ values between every combination fo variables for each subject. Here,
we use hierarchical clustering to identify which groups of subjects have the
most similar patterns in their VE results.

Briefly, we use these lists of R2 values to calculated the pairwise Euclidean
distances between each subject and performed hierarchical clustering with the
*dist* and *hclust* functions, respectively. Note, Euclidean distance is a valid
distance metric for our purposes, since the R2 values for all Zephyr and Kubios
metrics are mapped to the same range [0,1].

There are 20 multiscale entropy (MSE) variables generated by Kubios, all of
which have near-perfect correlation with each other and have almost no
correlation with any other variables. We excluded these variables since they
tend to drive clustering and overshadow the effects of all other variables.

Lastly, following our initial clustering we identified five subjects (two CRIC
and three CRIC/T2DM) that were substantially separated from all others. Upon
closer inspection, we found several of these subjects had Kubios data with a
high number of artifacts, so we exclude these subjects and repeated the
clustering analysis.
```{r load_clust_exclusion_list}
# Load the list of SubjectIDs for the subjects we're excluding
exclusion_list.clust =
    read_tsv(file.path(data_dir, "RAW_DATA/Clust_SubjectID_Exclude_List.txt"),
             col_types = "c") %>% 
    pull(SubjectID)
```
```{r hclust_ve_by_subject}
# The dist and hclust function require a matrix as input
zephyr_and_kubios.matrix_for_clust.exclude_MSE = 
    zephyr_and_kubios.var_explained_by_subject %>% 
    # Skip the subjects excluded for missing data
    filter(!(SubjectID %in% exclusion_list.clust),
           # skip cases where the same variable was compared to itself (VE will
           # always be 1)
           factor.1 != factor.2) %>% 
    #Exclude the MSE data
    filter(!grepl("MSE", factor.1),
           !grepl("MSE", factor.2)) %>% 
    # Remove reciprocal comparisons (i.e. 'a vs b' and 'b vs a' are the same
    # comparisons). Sort names of two factors being compared lexically, so the
    # original and reciprocal comparisons will have the same name. Then remove
    # all duplicate entries (reciprocal should have same value as original). If
    # for some reason the reciprocal has a different value than the original,
    # whichever comparison appears first is the one that is retained. This
    # likely has a minor impact as differences between reciprocal and original
    # comparisons are likely due to rounding errors and should be minimal.
    mutate(Comparison = paste(pmin(as.character(factor.1), as.character(factor.2)),
                              pmax(as.character(factor.1), as.character(factor.2)),
                              sep=" vs ")) %>% 
    select(Comparison, SubjectID, variability.explained) %>%
    distinct(Comparison, SubjectID, .keep_all = TRUE) %>% 
    spread(SubjectID, variability.explained) %>% 
    remove_rownames() %>% 
    column_to_rownames(var="Comparison") %>% 
    as.matrix()

# Perform hierarchical clustering
zephyr_and_kubios.hclust_results.exclude_MSE =
    zephyr_and_kubios.matrix_for_clust.exclude_MSE %>%
    t() %>% dist() %>%
    hclust()
```




# Time vs Subject (TvS) Contribution to Variance

For the Zephyr data we used the method we described in our previous work
([Skarke et al., 2017](https://pubmed.ncbi.nlm.nih.gov/29215023/)) to calculate
the contributions of time and subject to the total variability within each
metric. By comparing the contributions to variability from these two sources, we
can identify which metrics appear to be more strongly-driven by a time effect,
or an inter-individual effect.

## Method for Contribution to Variance Calculation

The contribution to variance calculation is analogous to the $R^2$ calculation
from linear regression, which we interpret as the fraction of variability in the
data explained by the regression model. We use the following function to perform
this calculation:
```{r define_tvs_function}
#' Variance contribution analysis
#'
#' This function calculates the variance contribution of the given grouping
#' variable across all of the data types present in the input table. We
#' ultimately want to calculate 1 - (residual sum of squares / total sum of squares).
#' However, we can calculate the equivalent measure (explained sum of squares /
#' total sum of squares) more directly.
#'
#' @param input_table Dataframe containing the data for the calculation. Must
#'   have two columns labeled Data.type and Signal, in addition to the grouping
#'   columns specified in the other argument.
#' @param group_cols String  or vector of strings containing the name(s) of the
#'   column(s) in input_table to use for grouping the data. The function is
#'   effectively calculating the contribution of the grouping variable(s) to the
#'   variability in various input data types. NOTE: if multiple grouping columns
#'   are given, this function does not calculate the individual contributions of
#'   each grouping to the variability. Rather, this functions uses all of the
#'   grouping columns together to partition the data.
#'
#' @return
#' @export
#'
#' @examples
calculate_variance_contribution <- function(input_table, group_cols) {
    
    # Get indices of grouping columns
    group_col_index = which(colnames(input_table) %in% group_cols)
    
    # Check to make sure all grouping columns are present in the input table
    if(length(group_col_index) != length(group_cols)) {
        # Identify the grouping columns not found in the input table
        cols_not_found = group_cols[which(!(group_cols %in% colnames(input_table)))]
        stop(cat("Error: The following grouping column(s) were not found in the input table:",
                 paste0("       \"", paste(group_cols, collapse="\",\""), "\""),
                 sep="\n"))
    }
    
    var_contrib_output = 
        input_table %>% 
        # Combine the grouping columns to make a single grouping variable
        unite(Group, all_of(group_col_index), sep=".") %>% 
        # Reduce table to just the columns needed for the calculation
        select(Data.type, Group, Signal) %>% 
        na.omit() %>% 
        # Calculate avg across all data
        group_by(Data.type) %>% 
        mutate(Total.Mean = mean(Signal, na.rm =TRUE)) %>%
        ungroup() %>% 
        # Calculate avg across all data and within each group and weighted,
        # squared differences between group and total means
        group_by(Data.type, Group) %>% 
        mutate(Group.Mean = mean(Signal, na.rm=TRUE),
               Group.num = sum(!is.na(Signal)),
               Weighted_group_total.Sq_Diff = Group.num * (Group.Mean - Total.Mean)^2) %>% 
        ungroup() %>% 
        # Calculate the squared differences from entire sample mean
        group_by(Data.type) %>% 
        mutate(Total.Sq_Diff = (Signal - Total.Mean)^2) %>%
        ungroup() %>% 
        # Sum these across all data and within each group
        group_by(Data.type, Group) %>% 
        summarise(Sum.Total.Sq_Diff = sum(Total.Sq_Diff),
                  Weighted_group_total.Sq_Diff = unique(Weighted_group_total.Sq_Diff),
                  .groups = "drop") %>% 
        # Calculate the following values:
        #    Total sum of squares (TSS)
        #    Explained sum of squares (ESS)
        # As well as the following ratio:
        #    (ESS / TSS)
        group_by(Data.type) %>% 
        mutate(TSS = sum(Sum.Total.Sq_Diff),
               ESS = sum(Weighted_group_total.Sq_Diff),
               Percentages = 100 * (ESS / TSS)) %>% 
        select(Data.type, Percentages) %>% 
        ungroup() %>% 
        distinct()
    
    return(var_contrib_output)
}
```

This function performs the following operations. First, we calculate the sum of
the squared differences between each data point and the mean of all data
($SS_{Total}$):

$$ SS_{Total} = \sum_{i=1}^{n}{(Y_i - \bar{Y})^2} $$
where $i$ refers to a specific measurement (e.g. HRV for subject 2 measured at
2:53 PM on February 17th, 2020), $Y_i$ is the value for the $i^\mathrm{th}$
measurement, $n$ is the total number of measurements, and $\bar{Y}$ is the mean
value across all $n$ measurements.

Next, we group the data by either clock hour (0-23) or subject, and calculate
the sum of the squared differences between the means of each group-level and the
mean of all data ($SS_{Group}$):

$$ SS_{Group} = \sum_{j=1}^{m}{(\bar{Y_j} - \bar{Y})^2} $$
where $j$ indexes a specific value for the grouping variable (a subject ID, or
clock hour), $m$ is the total number of different values for the grouping
variable (e.g. $m$ = 24 for clock hour), and $\bar{Y_j}$ is the mean measurement
value across all measurements for the $j^\mathrm{th}$ group.

Finally, we calculate the contribution of time or subject to total variability
as the ratio of $SS_{Group}$ and $SS_{Total}$.

## Perform Time vs Subject Contribution to Variance Calculations for Zephyr Data

For each cohort, we run the *calculate_variance_contribution* function on the
Zephyr BioPatch metrics grouped by time (i.e. clock hour [0-23]) and by subject
ID. By comparing these results, we can identify whether a given Zephyr metric
appears to be driven more by time, by subject, or equally by both.
```{r run_tvs_by_cohort, eval=FALSE}
# The calculate_variance_contribution() function expects the input data table to
# contain two primary columns: Data.type (e.g. Activity, HRV) and Signal (the
# actual data values). The table should also contain one or more grouping
# columns, used to partition the data for the variance contribution analysis.
# For doing a Time contribution analysis, the grouping columns should uniquely
# identify the time by hour. For a Subject contribution analysis, the grouping
# columns should uniquely identify each subject.

#Start with just the zephyr biopatch data binned by minute
zephyr_data.formatted_for_TvS =
    merged_zephyr_data.for_VE %>% 
    # Filter out subjects missing most of their data
    filter(!(SubjectID %in% exclusion_list.ve)) %>%
    # The calculate_variance_contribution() function expects
    # categorical grouping variables.
    mutate(ClockHour = as.integer(floor(ClockHour))) %>%
    pivot_longer(cols = -c(SubjectID, Cohort, TimeSubjectIndex, ClockHour, Time_of_day),
                 names_to = "Data.type", values_to = "Signal")

zephyr_data.TvS_results =
    zephyr_data.formatted_for_TvS %>% 
    # nest_by returns a rowwise tibble, so the mutate is performed separately on
    # each Measurement.
    nest_by(Cohort, .key = "data") %>% 
    mutate(TvS_results.Time =
               list(calculate_variance_contribution(data, "ClockHour") %>% 
                        rename(Time = Percentages)),
           TvS_results.Subject =
               list(calculate_variance_contribution(data, "SubjectID") %>% 
                        rename(Subject = Percentages)),
           TvS_results.merge = 
               list(full_join(TvS_results.Time, TvS_results.Subject,
                              by = "Data.type"))
    ) %>% 
    select(-data, -TvS_results.Time, -TvS_results.Subject) %>% 
    unnest(cols = TvS_results.merge) %>% 
    ungroup()
```

Save the TvS results to a tab-delimited text file.
```{r save_tvs_by_cohort, eval=FALSE}
write_tsv(zephyr_data.TvS_results,
          file.path(data_dir, "PROCESSED_DATA/TVS_RESULTS/Time_v_Subject.Zephyr.by_cohort.txt"))
```
<!-- Load the previously-generated Time vs Subject results. This speeds up
     knitting, since it avoids re-running the above code each time. -->
```{r load_tvs_by_cohort, echo=FALSE}
zephyr_data.TvS_results =
    read_tsv(file.path(data_dir, "PROCESSED_DATA/TVS_RESULTS/Time_v_Subject.Zephyr.by_cohort.txt"),
             col_types =
                 cols(Cohort = readr::col_factor(levels = cohort_levels),
                      Data.type = col_character(),
                      .default = col_double()
                 ))
```

# Sample Size Estimates

Through the [two-way ANOVA analyses](#two-way-anova-analysis), we identified
several metrics with significant differences between cohorts. These cardiac
metrics may provide mechanistic insight to factors affecting CKD severity, or
they may serve as useful predictors during the earlier stages of CKD onset.
Here, we perform sample-size estimates for future studies which seek to
replicate our findings, or are designed to specifically test the utility of
these metrics.

We begin by calculating the day and night means and standard deviations for each
subject. Next, we pool the standard deviations across all subjects when grouping
them by cohort, time of day, or all six of the cohort x time of day
combinations.
```{r calc_pooled_variances_for_anova_effects}
# Helper function to calculate pooled standard deviations, given a vector of
# standard deviations, and the number of observations contributing to each
# standard deviation.
pool_sds <- function(SD, N)
{
    sqrt( sum( (N - 1) * SD^2 ) / sum(N - 1))
}

pwr_calc_means_sds_pooled_sds =
    bind_rows(merged_zephyr_data.for_VE %>%
                  gather(Measurement, Value, -SubjectID:-Time_of_day) %>% 
                  # Remove HRV data for subject missing all data at night
                  filter(!(Measurement == "HRV" & SubjectID %in% exclusion_list.anova)),
              filtered.merged_kubios_results.for_VE %>%
                  gather(Measurement, Value, -SubjectID:-Time_of_day)
    ) %>% 
    # Exclude subjects missing most of their data
    filter(!(SubjectID %in% exclusion_list.ve)) %>%
    group_by(Measurement) %>% 
    # Calculate means and SDs within subjects first to account for within subject
    # effects.
    group_by(SubjectID, Cohort, Time_of_day, .add=TRUE) %>%
    summarise(Mean = mean(Value, na.rm=TRUE),
              SD = sd(Value, na.rm=TRUE),
              N = sum(!is.na(Value)),
              .groups = "drop_last") %>%
    rename(Value = Mean) %>%
    ungroup(SubjectID, Cohort) %>%
    group_by(Time_of_day, .add=TRUE) %>% 
    mutate(ToD.Mean = mean(Value, na.rm = T),
           ToD.SD = sd(Value, na.rm = T),
           ToD.Pooled_SD = pool_sds(SD, N),
           ToD.N = sum(!is.na(Value))) %>% 
    ungroup(Time_of_day) %>% 
    group_by(Cohort, .add=TRUE) %>% 
    mutate(Cohort.Mean = mean(Value, na.rm = T),
           Cohort.SD = sd(Value, na.rm = T),
           Cohort.Pooled_SD = pool_sds(SD, N),
           Cohort.N = sum(!is.na(Value))) %>% 
    ungroup(Cohort) %>% 
    group_by(Cohort, Time_of_day, .add=TRUE) %>% 
    mutate(Interaction.Mean = mean(Value, na.rm = T),
           Interaction.SD = sd(Value, na.rm = T),
           Interaction.Pooled_SD = pool_sds(SD, N),
           Interaction.N = sum(!is.na(Value))) %>% 
    ungroup(Cohort, Time_of_day) %>% 
    mutate(Total.Mean = mean(Value, na.rm = T),
           Total.SD = sd(Value, na.rm = T),
           Interaction.Pooled_SD = pool_sds(SD, N),
           Total.N = sum(!is.na(Value))) %>% 
    ungroup() %>% 
    select(Measurement, Time_of_day, Cohort, ends_with(".Mean"), ends_with(".SD"),
           ends_with(".Pooled_SD"), ends_with(".N")) %>% 
    distinct() %>% 
    gather(Metric, Value, -Measurement, -Time_of_day, -Cohort) %>% 
    separate(Metric, c("Effect", "Metric2"), sep="\\.") %>% 
    mutate(Group = 
               case_when(
                   Effect == "Cohort" ~ as.character(Cohort),
                   Effect == "ToD" ~ as.character(Time_of_day),
                   Effect == "Interaction" ~ paste0(as.character(Cohort), ":", as.character(Time_of_day)),
                   Effect == "Total" ~ "Total",
                   TRUE ~ NA_character_
               ),
           Group = factor(Group, levels = c("CKD", "CKD/T2DM", "Healthy control",
                                            "Day", "Night", "CKD:Day", "CKD:Night",
                                            "CKD/T2DM:Day", "CKD/T2DM:Night",
                                            "Healthy control:Day", "Healthy control:Night",
                                            "Total")),
           Effect = factor(Effect, levels = c("Cohort", "ToD", "Interaction", "Total"))
    ) %>% 
    arrange(Measurement, Group) %>% 
    select(-Time_of_day, -Cohort) %>% 
    distinct() %>% 
    spread(Metric2, Value)

# Calculate pooled variances for Cohort, Time of Day, and Interaction Effects
pwr_calc_means_sds_pooled_sds =
    pwr_calc_means_sds_pooled_sds %>% 
    group_by(Measurement, Effect) %>% 
    mutate(Pooled2 = pool_sds(Pooled_SD, N),
           Pooled_SD = pool_sds(SD, N)) %>%
    ungroup() %>% 
    # We don't want to do any pooling of the SD calculated across all data
    mutate(Pooled_SD = case_when(Effect == "Total" ~ SD,
                                 TRUE ~ Pooled_SD))
```

Finally, we use these pooled standard deviations as estimates for the
within-effect variances and perform separate sample size estimates for
significant cohort effects, time of day effects, and interaction effects. These
sample size estimates are likely conservative, as a two-way ANOVA has more power
than three separate one-way ANOVA tests for each main effect and the interaction
terms. 

We focus on four metrics: the mean RR interval, the standard deviation of the RR
interval (i.e. the SDNN), and the $\alpha\mathrm{2}$ detrended fluctuation
analysis ($\alpha\mathrm{2}$-DFA) from the Kubios data, and the HRV (i.e. the
SDNN) measured directly by the Zephyr BioPatch.

For each of these metrics, we estimate the sample size in two different ways.
First, we estimate samples sizes for fixed effect sizes based on clinically
relevant differences from the scientific literature: 100 ms for the mean RR
interval, 50 ms for both SDNN measures, and 0.14 for the $\alpha\mathrm{2}$-DFA.
Second, we estimate samples sizes to detect changes of 0.5 standard deviations
in each metric. We estimate samples sizes in both of these cases using a desired
power of 0.9 and significance level 0.05.
```{r fixed_effect_size_sample_sizes}
effect_sizes =
    tibble(Measurement = c("Mean_RR__(ms)...Time_Domain_Results...Statistical_parameters",
                           "HRV",
                           "STD_RR_(ms)...Time_Domain_Results...Statistical_parameters",
                           "alpha_2...Nonlinear_Results...Detrended_fluctuation_analysis"),
           Target_effect_size = c(100, 50, 50, 0.14))

fixed_effect_size_sample_size_results =
    right_join(pwr_calc_means_sds_pooled_sds,
               effect_sizes,
               by = "Measurement") %>% 
    filter(Group != "Total") %>% 
    mutate(SD = Pooled_SD) %>% 
    group_by(Measurement, Effect, SD, Target_effect_size) %>% 
    summarize(Num_groups = n(), .groups = "drop") %>% 
    # Create arguments for power.anova.test() function
    mutate(groups = Num_groups,
           between.var= (Target_effect_size^2) / Num_groups,
           within.var = SD^2,
           power = 0.9,
           sig.level = 0.05) %>% 
    select(Measurement, Effect, Num_groups, Target_effect_size, SD, groups:sig.level) %>% 
    nest_by(Measurement, Effect, Num_groups, Target_effect_size, SD, .key = "data") %>% 
    mutate(sample_size_results =
               purrr::pmap(data, power.anova.test)) %>% 
    summarise(broom::tidy(sample_size_results), .groups = "keep") %>% 
    mutate(Effect.Sample_size = ceiling(n)) %>% 
    ungroup(-Measurement) %>% 
    # We get a sample size estimate for the Cohort, Interaction, and Time of Day
    # effects. If we wanted an experiment powered to detect significant effects
    # for all three, we can take the max sample size estimate across all the
    # different effects.
    mutate(Experiment.Sample_size = max(Effect.Sample_size)) %>% 
    ungroup()

fixed_effect_size_sample_size_results2 =
    right_join(pwr_calc_means_sds_pooled_sds,
           effect_sizes,
           by = "Measurement") %>% 
    filter(Group != "Total") %>% 
    mutate(SD = Pooled2) %>% 
    group_by(Measurement, Effect, SD, Target_effect_size) %>% 
    summarize(Num_groups = n(), .groups = "drop") %>% 
    # Create arguments for power.anova.test() function
    mutate(groups = Num_groups,
           between.var= (Target_effect_size^2) / Num_groups,
           within.var = SD^2,
           power = 0.9,
           sig.level = 0.05) %>% 
    select(Measurement, Effect, Num_groups, Target_effect_size, SD, groups:sig.level) %>% 
    nest_by(Measurement, Effect, Num_groups, Target_effect_size, SD, .key = "data") %>% 
    mutate(sample_size_results =
               purrr::pmap(data, power.anova.test)) %>% 
    summarise(broom::tidy(sample_size_results), .groups = "keep") %>% 
    mutate(Effect.Sample_size = ceiling(n)) %>% 
    ungroup(-Measurement) %>% 
    # We get a sample size estimate for the Cohort, Interaction, and Time of Day
    # effects. If we wanted an experiment powered to detect significant effects
    # for all three, we can take the max sample size estimate across all the
    # different effects.
    mutate(Experiment.Sample_size = max(Effect.Sample_size)) %>% 
    ungroup()
```

```{r sd_based_sample_sizes}
one_half_sd_sample_size_results =
    right_join(pwr_calc_means_sds_pooled_sds,
           effect_sizes,
           by = "Measurement") %>% 
    filter(Group != "Total") %>% 
    # Set target effect size to one-half a standard deviation
    mutate(SD = Pooled2,
           Target_effect_size = 0.5*SD) %>% 
    group_by(Measurement, Effect, SD, Target_effect_size) %>% 
    summarize(Num_groups = n(), .groups = "drop") %>% 
    # Create arguments for power.anova.test() function
    mutate(groups = Num_groups,
           between.var= (Target_effect_size)^2, #  / Num_groups
           within.var = SD ^2,
           power = 0.9,
           sig.level = 0.05) %>% 
    select(Measurement, Effect, Num_groups, Target_effect_size, SD, groups:sig.level) %>% 
    nest_by(Measurement, Effect, Num_groups, Target_effect_size, SD, .key = "data") %>% 
    mutate(sample_size_results =
               purrr::pmap(data, power.anova.test)) %>% 
    summarise(broom::tidy(sample_size_results), .groups = "keep") %>% 
    mutate(Effect.Sample_size = ceiling(n)) %>% 
    ungroup(-Measurement) %>% 
    # We get a sample size estimate for the Cohort, Interaction, and Time of Day
    # effects. If we wanted an experiment powered to detect significant effects
    # for all three, we can take the max sample size estimate across all the
    # different effects.
    mutate(Experiment.Sample_size = max(Effect.Sample_size)) %>% 
    ungroup()
```


# Session Info

For reproducibility, here are the versions and details for the R installation
and packages used to generate this markdown document.

```{r sessioninfo, echo=FALSE}
session_info_output = sessioninfo::session_info()
# Remove names of local paths from session info output.
session_info_output$packages$library = NULL
# Strip director from pandoc version info listed in output
session_info_output$platform$pandoc =
    stringr::str_remove(session_info_output$platform$pandoc, " @ C:.*/")
session_info_output
```